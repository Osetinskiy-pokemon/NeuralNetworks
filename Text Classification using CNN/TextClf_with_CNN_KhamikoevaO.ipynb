{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiv5onaiowLr"
      },
      "source": [
        "# 6. Классификация текстов при помощи сверточных сетей\n",
        "\n",
        "__Автор__: Никита Владимирович Блохин (NVBlokhin@fa.ru)\n",
        "\n",
        "Финансовый университет, 2020 г. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from functools import lru_cache"
      ],
      "metadata": {
        "id": "SNrl1S5Wtrlg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx75RigN8xIJ"
      },
      "source": [
        "## 1. Представление и предобработка текстовых данных в виде последовательностей"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaWSvUHlvLbY",
        "outputId": "6ab239c7-604e-4c4c-a9bc-dbac2fd22c12"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LScKIAey9dAM"
      },
      "source": [
        "1.1 Представьте первое предложение из строки `text` как последовательность из индексов слов, входящих в это предложение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "phEw721T9SYW"
      },
      "outputs": [],
      "source": [
        "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.lower()"
      ],
      "metadata": {
        "id": "S-WhF-Jst9xR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_from_text = list(set(nltk.word_tokenize(text.replace(\".\", \"\")))) #берем все слова, которые встречаются в тексте (список уникальных слов)\n",
        "words_with_index = {j: i for i, j in enumerate(words_from_text)} #проставляем явный индекс словам\n",
        "words_with_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqnVMLoMylLl",
        "outputId": "db4956a6-fb42-456a-bc7e-8f445d46c48d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'your': 0,\n",
              " 'libtorch': 1,\n",
              " 'note': 2,\n",
              " 'preferences': 3,\n",
              " 'and': 4,\n",
              " 'available': 5,\n",
              " 'is': 6,\n",
              " 'run': 7,\n",
              " 'the': 8,\n",
              " 'command': 9,\n",
              " 'select': 10,\n",
              " 'c++': 11,\n",
              " 'currently': 12,\n",
              " 'supported': 13,\n",
              " 'pytorch': 14,\n",
              " 'version': 15,\n",
              " 'most': 16,\n",
              " 'stable': 17,\n",
              " 'install': 18,\n",
              " 'for': 19,\n",
              " 'only': 20,\n",
              " 'represents': 21,\n",
              " 'that': 22,\n",
              " 'tested': 23,\n",
              " 'of': 24}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[words_with_index[word] for word in nltk.word_tokenize(nltk.sent_tokenize(text)[0].replace(\".\", \"\"))] # получаем индексы слов из словаря уникальных слов для 1 предложения"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwnoof7l01O0",
        "outputId": "76671b3b-f81a-4ed3-a833-c200a887bedb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 0, 3, 4, 7, 8, 18, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSFQCPtD9x5J"
      },
      "source": [
        "1.2 Представьте первое предложение из строки `text` как последовательность векторов, соответствующих индексам слов. Для представления индекса в виде вектора используйте унитарное кодирование. В результате должен получиться двумерный тензор размера `количество слов в предложении` x `количество уникальных слов`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RZS4XLV0-buf"
      },
      "outputs": [],
      "source": [
        "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.lower()"
      ],
      "metadata": {
        "id": "DHWpbUFJXZ8N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_from_text = list(set(nltk.word_tokenize(text.replace(\".\", \"\")))) #берем все слова, которые встречаются в тексте (список уникальных слов)\n",
        "words_with_index = {j: i for i, j in enumerate(words_from_text)} #проставляем явный индекс словам"
      ],
      "metadata": {
        "id": "KHXcktEhXbp7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_from_first_sentence = nltk.word_tokenize(nltk.sent_tokenize(text.replace(\".\", \"\"))[0]) #берем слова из первого предложения"
      ],
      "metadata": {
        "id": "5XzJpzX9VkIA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = torch.zeros(len(words_from_first_sentence), len(words_from_text)) # кол-во строк = кол-во слов в предложении, кол-во столбцов = кол-во уникальных слов\n",
        "indices = [(i, words_with_index[j]) for i, j in enumerate(words_from_first_sentence)]\n",
        "vectors[list(zip(*indices))] = 1\n",
        "vectors\n",
        "# происходит one hot encoding - т.е. если слово встречается в предложении, то проставляется 1 в позиции в словаре уникальных слов"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq20TVceVkfU",
        "outputId": "570fc5de-feb9-44d5-d423-3df3d1c2fb17"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZvQKHYA-mJN"
      },
      "source": [
        "1.3 Решите задачу 1.2, используя модуль `nn.Embedding`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "embeds = nn.Embedding(num_embeddings=len(words_from_text), embedding_dim=len(words_from_text)) # слой, который на вход принимает индексы слов, а на выход полезные признаки\n",
        "indices = torch.tensor([words_with_index[i] for i in words_from_first_sentence]) # индексы слов из первого предложения\n",
        "embeds(indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZRZUHBAZsjP",
        "outputId": "84d536f3-bf59-4ffa-b06f-d49235ae0d98"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 9.0682e-01, -4.7551e-01, -8.7074e-01,  1.4474e-01,  1.9029e+00,\n",
              "          3.9040e-01, -3.9373e-02, -8.0147e-01, -4.9554e-01, -3.6151e-01,\n",
              "          5.8511e-01, -1.1560e+00, -1.4336e-01, -1.9474e-01, -8.5563e-02,\n",
              "          1.3945e+00,  5.9690e-01, -4.8285e-01, -3.6610e-01, -1.3271e+00,\n",
              "          1.6953e+00,  2.0655e+00, -2.3396e-01,  7.0732e-01,  5.8005e-01],\n",
              "        [-1.1258e+00, -1.1524e+00, -2.5058e-01, -4.3388e-01,  8.4871e-01,\n",
              "          6.9201e-01, -3.1601e-01, -2.1152e+00,  3.2227e-01, -1.2633e+00,\n",
              "          3.4998e-01,  3.0813e-01,  1.1984e-01,  1.2377e+00,  1.1168e+00,\n",
              "         -2.4728e-01, -1.3527e+00, -1.6959e+00,  5.6665e-01,  7.9351e-01,\n",
              "          5.9884e-01, -1.5551e+00, -3.4136e-01,  1.8530e+00,  7.5019e-01],\n",
              "        [-4.9968e-01, -1.0670e+00,  1.1149e+00, -1.4067e-01,  8.0575e-01,\n",
              "         -9.3348e-02,  6.8705e-01, -8.3832e-01,  8.9182e-04,  8.4189e-01,\n",
              "         -4.0003e-01,  1.0395e+00,  3.5815e-01, -2.4600e-01,  2.3025e+00,\n",
              "         -1.8817e+00, -4.9727e-02, -1.0450e+00, -9.5650e-01,  3.3532e-02,\n",
              "          7.1009e-01,  1.6459e+00, -1.3602e+00,  3.4457e-01,  5.1987e-01],\n",
              "        [-2.6133e+00, -1.6965e+00, -2.2824e-01,  2.7995e-01,  2.4693e-01,\n",
              "          7.6887e-02,  3.3801e-01,  4.5440e-01,  4.5694e-01, -8.6537e-01,\n",
              "          7.8131e-01, -9.2679e-01, -2.1883e-01, -2.4351e+00, -7.2915e-02,\n",
              "         -3.3986e-02,  9.6252e-01,  3.4917e-01, -9.2146e-01, -5.6195e-02,\n",
              "         -6.2270e-01, -4.6372e-01,  1.9218e+00, -4.0255e-01,  1.2390e-01],\n",
              "        [-1.3962e+00, -6.6144e-02, -3.5836e-01, -1.5616e+00, -3.5464e-01,\n",
              "          1.0811e+00,  1.3148e-01,  1.5735e+00,  7.8143e-01, -1.0787e+00,\n",
              "         -7.2091e-01,  1.4708e+00,  2.7564e-01,  6.6678e-01, -9.9439e-01,\n",
              "         -1.1894e+00, -1.1959e+00, -5.5963e-01,  5.3347e-01,  4.0689e-01,\n",
              "          3.9459e-01,  1.7151e-01,  8.7604e-01, -2.8709e-01,  1.0216e+00],\n",
              "        [-7.4395e-02, -1.0922e+00,  3.9203e-01,  5.9453e-01,  6.6227e-01,\n",
              "         -1.2063e+00,  6.0744e-01, -5.4716e-01,  1.1711e+00,  9.7496e-02,\n",
              "          9.6337e-01,  8.4032e-01, -1.2537e+00,  9.8684e-01, -4.9466e-01,\n",
              "         -1.2830e+00,  9.5522e-01,  1.2836e+00, -6.6586e-01,  5.6513e-01,\n",
              "          2.8770e-01, -3.3375e-02, -1.0619e+00, -1.1443e-01, -3.4334e-01],\n",
              "        [ 1.8024e+00, -1.0597e+00,  3.4028e+00, -5.6867e-01, -4.7549e-01,\n",
              "          1.7432e+00, -2.0441e-01, -3.1641e-01,  1.2937e+00,  1.3453e+00,\n",
              "          1.9394e-01,  1.5717e+00, -3.8274e-01,  1.3951e+00,  3.4275e-01,\n",
              "         -1.6045e+00, -5.8731e-01,  6.0039e-01,  4.3780e-01, -9.6455e-02,\n",
              "          3.3027e-01, -1.8752e-01, -1.4271e+00,  5.9255e-01, -1.1582e+00],\n",
              "        [ 1.5713e+00,  1.9161e-01,  3.7994e-01, -1.4476e-01,  6.3762e-01,\n",
              "         -2.8129e-01, -1.3299e+00, -1.4201e-01, -5.3415e-01, -5.2338e-01,\n",
              "          8.6150e-01, -8.8696e-01,  8.3877e-01,  1.1529e+00, -1.7611e+00,\n",
              "         -1.4777e+00, -1.7557e+00,  7.6166e-02, -1.0786e+00,  1.4403e+00,\n",
              "         -1.1059e-01,  5.7686e-01, -1.6917e-01, -6.4025e-02,  1.0384e+00],\n",
              "        [ 2.7152e-01,  6.7158e-01,  1.8500e+00,  1.1910e+00, -5.8986e-01,\n",
              "          9.6469e-01, -1.5094e+00,  2.2557e+00,  1.2288e+00, -4.8546e-01,\n",
              "          4.5357e-01,  1.3514e+00,  4.3393e-01, -5.1325e-01, -1.8603e-01,\n",
              "          2.7566e-01,  1.0969e-01,  3.5942e-01, -7.5374e-01,  2.2940e-01,\n",
              "         -2.5444e-01,  1.5800e+00, -2.4436e-01, -1.1991e+00, -2.5686e-02],\n",
              "        [ 1.2532e+00, -4.4452e-01,  8.1845e-01, -8.1802e-01,  3.6032e-01,\n",
              "         -1.6146e+00, -2.4734e+00,  3.6156e-02, -3.4222e-01, -3.8169e-01,\n",
              "         -5.6879e-02,  8.4362e-01,  6.8287e-01,  3.3944e+00, -1.6688e+00,\n",
              "          5.1086e-01, -2.8599e-01,  3.3505e-01,  1.1719e+00,  1.2955e+00,\n",
              "          8.9086e-01, -4.8985e-01, -1.1727e+00, -6.8705e-01, -2.3349e+00],\n",
              "        [-7.4395e-02, -1.0922e+00,  3.9203e-01,  5.9453e-01,  6.6227e-01,\n",
              "         -1.2063e+00,  6.0744e-01, -5.4716e-01,  1.1711e+00,  9.7496e-02,\n",
              "          9.6337e-01,  8.4032e-01, -1.2537e+00,  9.8684e-01, -4.9466e-01,\n",
              "         -1.2830e+00,  9.5522e-01,  1.2836e+00, -6.6586e-01,  5.6513e-01,\n",
              "          2.8770e-01, -3.3375e-02, -1.0619e+00, -1.1443e-01, -3.4334e-01],\n",
              "        [-7.6447e-01,  2.4084e-01,  1.6643e-01, -2.2318e+00,  1.3892e+00,\n",
              "         -5.0233e-01,  1.6797e+00, -1.0240e+00,  1.6859e+00, -1.2177e+00,\n",
              "          7.6496e-01,  1.1971e+00, -7.1279e-01, -6.5576e-02,  2.2050e+00,\n",
              "          1.7852e+00, -1.1840e-02,  9.7967e-01, -1.0661e+00,  1.7720e+00,\n",
              "         -2.7926e-01, -2.7690e-01,  7.4893e-01, -6.4346e-01, -9.5176e-01],\n",
              "        [ 1.4628e+00, -6.2043e-01,  9.8839e-01, -4.3218e-01, -6.2322e-01,\n",
              "         -2.1625e-01, -4.8868e-01,  7.8696e-01,  1.0759e-01, -1.0715e+00,\n",
              "         -1.1665e-01, -1.0170e+00, -1.1980e+00,  4.7844e-01, -1.2295e+00,\n",
              "         -1.3700e+00,  1.5435e+00, -3.3207e-02, -4.1863e-01, -2.5560e-01,\n",
              "         -1.2923e-01, -5.4595e-02,  4.0835e-01,  1.1264e+00,  1.9351e+00],\n",
              "        [-5.2965e-01, -8.7331e-01,  4.2614e-03, -1.2579e+00, -1.0845e+00,\n",
              "          7.5298e-01,  3.2365e-01, -2.7501e-01,  1.3056e+00,  2.1175e-01,\n",
              "          2.7196e-01, -9.2684e-01, -2.7330e+00, -5.6417e-01, -2.7400e-01,\n",
              "          1.3978e-01,  5.0856e-01,  2.7710e-01, -9.8125e-01,  8.8885e-01,\n",
              "          1.5690e+00, -8.1853e-02, -3.4940e-01,  2.0243e-01, -2.8838e-01],\n",
              "        [-2.6133e+00, -1.6965e+00, -2.2824e-01,  2.7995e-01,  2.4693e-01,\n",
              "          7.6887e-02,  3.3801e-01,  4.5440e-01,  4.5694e-01, -8.6537e-01,\n",
              "          7.8131e-01, -9.2679e-01, -2.1883e-01, -2.4351e+00, -7.2915e-02,\n",
              "         -3.3986e-02,  9.6252e-01,  3.4917e-01, -9.2146e-01, -5.6195e-02,\n",
              "         -6.2270e-01, -4.6372e-01,  1.9218e+00, -4.0255e-01,  1.2390e-01],\n",
              "        [ 1.0077e+00,  1.0046e+00, -4.3352e-01, -1.2426e+00,  1.2846e+00,\n",
              "          2.4377e-01,  5.3037e-01, -1.4531e-02, -2.2357e+00,  1.4660e+00,\n",
              "         -1.2191e+00,  6.4423e-01,  3.9300e+00, -1.2442e-01,  2.9534e-01,\n",
              "          3.8265e-01, -5.4972e-01, -9.9404e-01,  1.3459e+00,  1.9457e+00,\n",
              "         -1.2904e+00, -2.3495e+00, -2.0689e+00,  9.0942e-01, -6.9462e-01],\n",
              "        [ 6.5730e-01, -3.1122e-01, -5.6200e-01, -4.8349e-01, -1.2721e+00,\n",
              "         -1.7402e-01,  5.5412e-01, -1.8166e-01, -2.3447e-01,  2.9420e-01,\n",
              "          7.9732e-01,  1.2642e+00,  9.3549e-01,  5.4546e-01, -1.5374e+00,\n",
              "          3.1244e-01,  7.4006e-01,  1.4502e+00,  4.1015e+00,  1.1182e+00,\n",
              "         -1.5668e+00, -6.9898e-01,  5.7439e-01,  1.2381e+00, -6.4054e-01],\n",
              "        [ 1.4830e-01,  2.4187e+00,  1.3279e+00, -2.6386e-01,  3.6447e-01,\n",
              "          2.5440e+00, -2.6895e+00,  2.4426e+00,  1.0375e-02, -5.6443e-01,\n",
              "         -9.1837e-01, -7.4956e-01, -9.4933e-02,  1.1009e+00,  1.3105e+00,\n",
              "         -2.9285e-01,  4.1808e-01, -1.6952e-01, -2.1749e+00,  7.2025e-01,\n",
              "          2.8545e-01,  2.2903e-01,  1.2833e+00, -1.3792e+00,  1.4042e+00],\n",
              "        [ 1.9595e+00, -1.1038e+00,  5.4114e-01,  1.5390e+00,  1.0860e+00,\n",
              "          1.2464e+00,  1.1508e-01,  1.6193e+00,  4.6369e-01,  1.3007e+00,\n",
              "          8.7323e-01,  6.5127e-02,  7.7324e-01, -9.7014e-01, -8.8768e-01,\n",
              "         -3.1832e-01, -3.3440e-01,  4.5428e-01,  4.9895e-01,  8.7800e-01,\n",
              "          3.8944e-01,  1.4625e+00,  4.7951e-01, -5.3340e-01, -3.4651e-02],\n",
              "        [ 1.0554e+00,  1.7784e-01, -2.3034e-01, -3.9175e-01,  5.4329e-01,\n",
              "         -3.9516e-01, -4.4622e-01,  7.4402e-01,  1.5210e+00,  3.4105e+00,\n",
              "         -1.5312e+00, -1.2341e+00,  1.8197e+00, -5.5153e-01, -5.6925e-01,\n",
              "          9.1997e-01,  1.1108e+00,  1.2899e+00, -1.4782e+00,  2.5672e+00,\n",
              "         -4.7312e-01,  3.3555e-01, -1.6293e+00, -5.4974e-01, -4.7983e-01],\n",
              "        [ 9.4041e-02, -2.0208e-01, -5.9524e-02,  2.0118e+00, -3.3679e-01,\n",
              "          3.2598e-01,  5.3520e-01,  1.9733e+00, -2.0751e-01, -3.0575e-02,\n",
              "          1.2673e-01,  5.5466e-03,  7.9434e-01,  4.0715e-01, -3.6090e-01,\n",
              "          1.3103e+00, -9.6505e-01,  8.8061e-01, -1.0247e-01, -6.7701e-01,\n",
              "         -4.1066e-01, -1.6186e+00,  5.0791e-01,  2.3230e+00,  2.2978e-01],\n",
              "        [-5.8550e-01, -1.7340e-01,  1.8348e-01,  1.3894e+00,  1.5863e+00,\n",
              "          9.4630e-01, -8.4368e-01, -6.1358e-01,  3.1593e-02, -4.9268e-01,\n",
              "          2.4841e-01,  4.3970e-01,  1.1241e-01,  6.4079e-01,  4.4116e-01,\n",
              "         -1.0231e-01,  7.9244e-01, -2.8967e-01,  5.2507e-02,  5.2286e-01,\n",
              "          2.3022e+00, -1.4689e+00, -1.5867e+00, -6.7309e-01,  8.7283e-01],\n",
              "        [-2.0106e-01, -1.1793e-01,  1.9220e-01, -7.7216e-01, -1.9003e+00,\n",
              "          1.3068e-01, -7.0429e-01,  3.1472e-01,  1.5739e-01,  3.8536e-01,\n",
              "          9.6715e-01, -9.9108e-01,  3.0161e-01, -1.0732e-01,  9.9846e-01,\n",
              "         -4.9871e-01,  7.6111e-01,  6.1830e-01,  3.1405e-01,  2.1333e-01,\n",
              "         -1.2005e-01,  3.6046e-01, -3.1403e-01, -1.0787e+00,  2.4081e-01],\n",
              "        [-1.6422e-01, -9.7147e-01, -1.0308e+00,  6.4728e-01, -1.9061e-01,\n",
              "          7.1665e-01, -2.0002e+00, -2.4097e+00,  2.1942e-01, -1.6989e+00,\n",
              "          1.3094e+00, -1.6613e+00, -5.4607e-01, -6.3018e-01, -6.3465e-01,\n",
              "          9.7466e-01,  2.0984e-01,  2.9890e-02,  1.7092e+00, -7.2576e-01,\n",
              "         -7.7354e-01,  5.9621e-01, -1.2504e+00,  1.1456e+00,  7.3934e-01],\n",
              "        [ 1.1648e+00,  9.2337e-01,  1.3873e+00, -8.8338e-01, -4.1891e-01,\n",
              "         -8.0483e-01,  5.6561e-01,  6.1036e-01,  4.6688e-01,  1.9507e+00,\n",
              "         -1.0631e+00, -7.7326e-02,  1.1640e-01, -5.9399e-01, -1.2439e+00,\n",
              "         -1.0209e-01, -1.0335e+00, -3.1264e-01,  2.4579e-01, -2.5964e-01,\n",
              "          1.1834e-01,  2.4396e-01,  1.1646e+00,  2.8858e-01,  3.8660e-01],\n",
              "        [ 3.5761e-02,  2.1601e-01, -9.1608e-01,  1.5599e+00, -3.1537e+00,\n",
              "         -5.6110e-01, -4.3030e-01, -3.3323e-01, -1.5464e+00, -1.4717e-02,\n",
              "          1.2251e+00,  1.5936e+00, -1.6315e+00, -5.6877e-02,  6.2966e-01,\n",
              "          2.7117e-01, -6.8598e-01, -1.0918e+00,  1.6797e+00, -8.8082e-01,\n",
              "          5.8003e-01,  3.6423e-01,  8.8134e-02, -1.3069e+00, -7.0637e-01],\n",
              "        [ 2.6830e-01, -2.0589e+00,  5.3402e-01, -5.3539e-01, -8.6366e-01,\n",
              "         -2.3494e-02,  1.1717e+00,  3.9869e-01, -1.9872e-01, -1.1559e+00,\n",
              "         -3.1667e-01,  9.4030e-01, -1.1470e+00,  5.5880e-01,  7.9176e-01,\n",
              "         -1.8468e-01, -7.3177e-01, -8.0652e-02, -9.8006e-01,  6.0491e-02,\n",
              "         -4.8895e-01, -8.1373e-01,  8.1999e-01, -6.3317e-01,  1.2948e+00]],\n",
              "       grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXjM7qEUNFY_"
      },
      "source": [
        "## 2. Классификация фамилий по национальности (ConvNet)\n",
        "\n",
        "Датасет: https://disk.yandex.ru/d/owHew8hzPc7X9Q?w=1\n",
        "\n",
        "2.1 Считать файл `surnames/surnames.csv`. \n",
        "\n",
        "2.2 Закодировать национальности числами, начиная с 0.\n",
        "\n",
        "2.3 Разбить датасет на обучающую и тестовую выборку\n",
        "\n",
        "2.4 Реализовать класс `Vocab` (токен = __символ__)\n",
        "  * добавьте в словарь специальный токен `<PAD>` с индексом 0\n",
        "  * при создании словаря сохраните длину самой длинной последовательности из набора данных в виде атрибута `max_seq_len`\n",
        "\n",
        "2.5 Реализовать класс `SurnamesDataset`\n",
        "  * метод `__getitem__` возвращает пару: <последовательность индексов токенов (см. 1.1 ), номер класса> \n",
        "  * длина каждой такой последовательности должна быть одинаковой и равной `vocab.max_seq_len`. Чтобы добиться этого, дополните последовательность справа индексом токена `<PAD>` до нужной длины\n",
        "\n",
        "2.6. Обучить классификатор.\n",
        "  \n",
        "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`. Рассмотрите два варианта: \n",
        "    - когда токен представляется в виде унитарного вектора и модуль `nn.Embedding` не обучается\n",
        "    - когда токен представляется в виде вектора небольшой размерности (меньше, чем размер словаря) и модуль `nn.Embedding` обучается\n",
        "\n",
        "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
        "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`\n",
        "\n",
        "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: прогнать несколько фамилий студентов группы через модели и проверить результат. Для каждой фамилии выводить 3 наиболее вероятных предсказания."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Считать файл surnames/surnames.csv."
      ],
      "metadata": {
        "id": "FUS1AK8mdTxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yNHQV7peRbe",
        "outputId": "7b992675-f6c0-4431-b6a6-ee297a2e4882"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "surname_df = pd.read_csv('/content/drive/MyDrive/ML_FU/Lab_6_embeddings/data/surnames.csv')\n",
        "surname_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6yTZq5qtdWIJ",
        "outputId": "a7403f83-b534-4634-87b8-7d35955aed42"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    surname nationality\n",
              "0  Woodford     English\n",
              "1      Coté      French\n",
              "2      Kore     English\n",
              "3     Koury      Arabic\n",
              "4    Lebzak     Russian"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e10fb29b-32b3-46f9-92ef-a7a9944be4c6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>surname</th>\n",
              "      <th>nationality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Woodford</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Coté</td>\n",
              "      <td>French</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Kore</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Koury</td>\n",
              "      <td>Arabic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lebzak</td>\n",
              "      <td>Russian</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e10fb29b-32b3-46f9-92ef-a7a9944be4c6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e10fb29b-32b3-46f9-92ef-a7a9944be4c6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e10fb29b-32b3-46f9-92ef-a7a9944be4c6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 Закодировать национальности числами, начиная с 0."
      ],
      "metadata": {
        "id": "_niwmQQZei5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labeler = LabelEncoder()\n",
        "surname_df[\"target\"] = labeler.fit_transform(surname_df[\"nationality\"])\n",
        "print(f\"classes: {len(labeler.classes_)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxoY8hyYekI5",
        "outputId": "d8ba249c-5fa1-4812-abc8-36b160f5c6d0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classes: 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "surname_df.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "M4-9qbQnlQzz",
        "outputId": "032d3290-6e10-4d6d-f76c-f095dbb5b058"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    surname nationality  target\n",
              "0  Woodford     English       4\n",
              "1      Coté      French       5"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee7dadd5-20f9-41e4-b102-879054161a7b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>surname</th>\n",
              "      <th>nationality</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Woodford</td>\n",
              "      <td>English</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Coté</td>\n",
              "      <td>French</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee7dadd5-20f9-41e4-b102-879054161a7b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ee7dadd5-20f9-41e4-b102-879054161a7b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ee7dadd5-20f9-41e4-b102-879054161a7b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4 Реализовать класс Vocab (токен = символ)\n",
        "\n",
        "\n",
        "*   добавьте в словарь специальный токен <PAD> с индексом 0\n",
        "*   при создании словаря сохраните длину самой длинной последовательности из набора данных в виде атрибута max_seq_len"
      ],
      "metadata": {
        "id": "vdNYzRslyPzd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZGfJX2NP1sw4"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "  \n",
        "  pad = \"<PAD>\"\n",
        "\n",
        "  def __init__(self, series):\n",
        "    unique_list = set()\n",
        "    max_seq_len = 0\n",
        "    #заполняем символами перечень и ищем максимальную длину\n",
        "    for i in map(str.lower, series):\n",
        "      unique_list.update(i)\n",
        "      max_seq_len = max(len(i), max_seq_len)\n",
        "\n",
        "    self.symbols = [self.pad, *unique_list] #в начало проставляем pad\n",
        "    self.max_seq_len = max_seq_len #максимальная длина\n",
        "    self.symb_ind = {ch: i for i, ch in enumerate(self.symbols)} #сивол - индекс\n",
        "\n",
        "  #кодирование последовательности\n",
        "  def encode(self, word):\n",
        "    word = word.lower() #если вдруг на вход поступит последовательность с заглавными буквами\n",
        "    indices = [self.symb_ind[symb] for symb in word]\n",
        "    # дополняем индексом служебного символа\n",
        "    indices += [self.symb_ind[self.pad]] * (self.max_seq_len - len(indices))\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "  #расшифровка закодированной последовательности\n",
        "  def decode(self, indices):\n",
        "    indices_2 = torch.nonzero(indices == self.symb_ind[self.pad], as_tuple=True)[0]\n",
        "    if len(indices_2):\n",
        "      indices = indices[:indices_2[0]]  #без служебных символов\n",
        "    return \"\".join(self.symbols[i] for i in indices)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Vocab(surname_df[\"surname\"])\n",
        "encode = vocab.encode(\"Khamikoeva\")\n",
        "print(encode, vocab.decode(encode))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKXaOlZtG3z2",
        "outputId": "6b4dbc69-6fe3-4c00-8c4f-08091a54640e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([32, 46, 39, 35,  9, 32, 25, 51, 20, 39,  0,  0,  0,  0,  0,  0,  0]) khamikoeva\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.5 Реализовать класс `SurnamesDataset`\n",
        "  * метод `__getitem__` возвращает пару: <последовательность индексов токенов (см. 1.1 ), номер класса> \n",
        "  * длина каждой такой последовательности должна быть одинаковой и равной `vocab.max_seq_len`. Чтобы добиться этого, дополните последовательность справа индексом токена `<PAD>` до нужной длины\n"
      ],
      "metadata": {
        "id": "wiTlRsfxImyJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GHjCRqQg1sw5"
      },
      "outputs": [],
      "source": [
        "class SurnamesDataset(Dataset):\n",
        "  def __init__(self, data, vocab, transform):\n",
        "    self.surname = data[\"surname\"].tolist()\n",
        "    if transform:\n",
        "      size = transform(self.surname[0]).size() # сохраняем размер элемента датасета\n",
        "      self.data = torch.vstack([transform(i) for i in self.surname]).view(len(self.surname), *size) # кодируем каждую фамилию, vstack - вдоль вертикали объединяем тензор, view - восстанавливаем размер\n",
        "    else:\n",
        "      self.data = self.surname\n",
        "    self.targets = torch.tensor(data[\"target\"], dtype=torch.long)\n",
        "\n",
        "    self.vocab = vocab\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    return self.data[i], self.targets[i]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_index(word):\n",
        "  return vocab.encode(word) #кодируем\n",
        "\n",
        "def one_hot(word):\n",
        "  vectors = torch.zeros(vocab.max_seq_len, len(vocab.symbols))\n",
        "  indices = [(i, vocab.symb_ind[symb]) for i, symb in enumerate(word.lower())]\n",
        "  vectors[list(zip(*indices))] = 1\n",
        "  return vectors"
      ],
      "metadata": {
        "id": "MVWhn-oZLnP4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#для задания 2.6\n",
        "# датасет для обучения без Embedding (слова закодированы one-hot методом)\n",
        "surname_one_hot = SurnamesDataset(surname_df, vocab, transform=one_hot)\n",
        "# датасет для обучения через Embedding\n",
        "surname_indices = SurnamesDataset(surname_df, vocab, transform=to_index)\n",
        "surname_one_hot[0], surname_indices[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXZ8vwQh3ynU",
        "outputId": "01768f3d-60b8-47de-ce5f-80c648371bd4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "           0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0.]]), tensor(4)),\n",
              " (tensor([34, 25, 25, 17, 14, 25, 48, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
              "  tensor(4)))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3 Разбить датасет на обучающую и тестовую выборку"
      ],
      "metadata": {
        "id": "0tWY0E-uQoZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_train_test(data, train_part): #делим на обучающую и тестовую выборки\n",
        "  train_size = round(train_part * len(data)) #размер обучающей\n",
        "  test_size = len(data) - train_size #размер тестовой\n",
        "  train, test = random_split(data, lengths=(train_size, test_size)) #делим\n",
        "  return train, test"
      ],
      "metadata": {
        "id": "c7pmfkXn4F6F"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "train_indices, test_indices = split_train_test(surname_indices, train_part=0.8)\n",
        "train_one_hot, test_one_hot = split_train_test(surname_one_hot, train_part=0.8)\n",
        "print(f'Train size: {len(train_indices)}, Test size: {len(test_indices)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLbMS8tnUYSj",
        "outputId": "7c158fa7-ccb3-4438-aeee-14bce866b63d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 8784, Test size: 2196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.6. Обучить классификатор.\n",
        "  \n",
        "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`. Рассмотрите два варианта: \n",
        "    - когда токен представляется в виде унитарного вектора и модуль `nn.Embedding` не обучается\n",
        "    - когда токен представляется в виде вектора небольшой размерности (меньше, чем размер словаря) и модуль `nn.Embedding` обучается\n",
        "\n",
        "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
        "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`"
      ],
      "metadata": {
        "id": "Qza2hESvVWLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SurnamesClassifier(nn.Module):\n",
        "  def __init__(self, vocab: Vocab, out_features: int, embed_dim: int=128, use_embedding: bool=True, debug: bool=False):\n",
        "    \n",
        "    super(SurnamesClassifier, self).__init__()\n",
        "        \n",
        "    self.use_embedding = use_embedding\n",
        "    self.debug = debug\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "    last_out_channels = 64 \n",
        "    adaptive_avg_pool = 8\n",
        "\n",
        "    self.embedding = nn.Embedding(num_embeddings=len(vocab.symbols), embedding_dim=embed_dim) # слой, который дает признаки из индексов слов\n",
        "    self.features = nn.Sequential(\n",
        "      nn.Conv1d(in_channels=embed_dim, out_channels=64, kernel_size=3), # сверточный слой, embed_dim - кол-во признаков от эмбеддинга, out_channels - выходные слои, kernel_size - вход уменьшится на 2 с каждой стороны\n",
        "      nn.BatchNorm1d(num_features=64), # иногда улучшает точность\n",
        "      nn.ReLU(), # функция активации\n",
        "      nn.MaxPool1d(kernel_size=2), # уменьшает размерность в 2 раза\n",
        "      nn.Conv1d(in_channels=64, out_channels=last_out_channels, kernel_size=3), # сверточный слой, кол-во каналов = кол-ву каналов из предыдущего сверточного слоя\n",
        "      nn.BatchNorm1d(num_features=last_out_channels), # повышение точности\n",
        "      nn.ReLU(), # функция активации\n",
        "      nn.MaxPool1d(kernel_size=2), # уменьшает размерность еще в 2 раза\n",
        "    )\n",
        "    self.avgpool = nn.AdaptiveAvgPool1d(adaptive_avg_pool) # слой для контроля размерности - указываем необходимую размерность\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Linear(last_out_channels * adaptive_avg_pool, 256), # линейный слой, где размерность на вход = 64*8\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(),\n",
        "      nn.Linear(256, out_features), # Линейный слой\n",
        "    )\n",
        "\n",
        "    if self.debug:\n",
        "      self.forward = self._debug_forward\n",
        "    else:\n",
        "      self.forward = self._forward\n",
        "\n",
        "  def _forward(self, x):\n",
        "    if self.use_embedding:\n",
        "      x = self.embedding(x)\n",
        "    else:\n",
        "      x = F.pad(x, (0, self.embed_dim - x.size(2), 0, 0), value=0)\n",
        "\n",
        "    x = x.reshape(x.size(0), x.size(2), x.size(1)) # ставит на место размерности (последняя и предпоследняя меняются местами)\n",
        "    x = self.features(x) # свертка\n",
        "    x = self.avgpool(x) # меняем размерность\n",
        "    x = torch.flatten(x, 1) # нужно привести к количеству измерений = 2\n",
        "    x = self.classifier(x) # линейные слои\n",
        "    return torch.log_softmax(x, dim=1) # наблюдение за ошибкой\n",
        "\n",
        "  def _debug_forward(self, x):\n",
        "    print(\"x: \", x.size())\n",
        "    if self.use_embedding:\n",
        "      x = self.embedding(x)\n",
        "      print(\"embedding: \", x.size())\n",
        "    else:\n",
        "      x = F.pad(x, (0, self.embed_dim - x.size(2), 0, 0), value=0)\n",
        "      print(\"pad: \", x.size())\n",
        "\n",
        "    x = x.reshape(x.size(0), x.size(2), x.size(1))\n",
        "    print(\"reshape: \", x.size())\n",
        "    x = self.features(x)\n",
        "    print(\"features: \", x.size())\n",
        "    x = self.avgpool(x)\n",
        "    print(\"avgpool: \", x.size())\n",
        "    x = torch.flatten(x, 1)\n",
        "    print(\"flatten: \", x.size())\n",
        "    x = self.classifier(x)\n",
        "    print(\"classifier: \", x.size())\n",
        "    return torch.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "FUNl6xsUVa1C"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  DEVICE = \"cuda\"\n",
        "else:\n",
        "  DEVICE = \"cpu\""
      ],
      "metadata": {
        "id": "GIvwiHkpb1JV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def on_cuda(device):\n",
        "#   return device == \"cuda\"\n",
        "\n",
        "def train_func(dataloader, model, loss_fn, optimizer, verbose, device):\n",
        "  \n",
        "  model.train()\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  avg_loss = 0\n",
        "\n",
        "  for batch, (x, y) in enumerate(dataloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    pred = model(x)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    avg_loss += loss\n",
        "    if batch % verbose == 0:\n",
        "      print(f'loss: {loss:>7f}  [{batch * len(x):>5d} / {size:>5d}]')\n",
        "\n",
        "    del x, y, pred, loss\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  return avg_loss / num_batches\n",
        "\n",
        "\n",
        "@torch.no_grad() # не считать градиенты для экономии памяти и времени\n",
        "def test_func(dataloader, model, loss_fn, device):\n",
        "  model.eval()\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  avg_loss, correct = 0, 0\n",
        "\n",
        "  for x, y in dataloader:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    pred = model(x)\n",
        "    avg_loss += loss_fn(pred, y)\n",
        "    correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    del x, y, pred\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  avg_loss /= num_batches\n",
        "  accuracy = correct / size\n",
        "  print(f'Accuracy: {accuracy}, Avg loss: {avg_loss} \\n')\n",
        "\n",
        "  return avg_loss, accuracy\n",
        "\n",
        "def common(model, loss_fn, optimizer, train_dataloader, epochs, test_dataloader, lr_scheduler=None, verbose: int=100, device: str='cpu'):\n",
        "    \n",
        "  train_losses = []\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}\\n\" + \"_\" * 40)\n",
        "    train_loss = train_func(train_dataloader, model, loss_fn, optimizer, verbose=verbose, device=device)\n",
        "    train_losses.append(train_loss.item())\n",
        "    if test_dataloader:\n",
        "      loss, acc = test_func(test_dataloader, model, loss_fn, device=device)\n",
        "      if lr_scheduler:\n",
        "        lr_scheduler.step(loss)\n",
        "    torch.cuda.empty_cache()\n",
        "  return train_losses"
      ],
      "metadata": {
        "id": "uEWuIXNOcJ21"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "common_net = SurnamesClassifier(vocab, len(labeler.classes_)).to(DEVICE)\n",
        "loss_fn = nn.NLLLoss()\n",
        "optimizer = optim.Adam(common_net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "UhJSS7Tbbz1k"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# One-Hot\n",
        "common_net.use_embedding = False\n",
        "hot = common(\n",
        "    epochs=10,\n",
        "    model=common_net,\n",
        "    loss_fn=loss_fn,\n",
        "    optimizer=optimizer,\n",
        "    train_dataloader=DataLoader(train_one_hot, batch_size=8, shuffle=True),\n",
        "    test_dataloader=DataLoader(test_one_hot, batch_size=512),\n",
        "    verbose=500,\n",
        "    device=DEVICE\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBxvpt4YvIdB",
        "outputId": "d9022cf4-7732-462b-87de-bfe1ba3f4e04"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "________________________________________\n",
            "loss: 2.915266  [    0 /  8784]\n",
            "loss: 1.868670  [ 4000 /  8784]\n",
            "loss: 1.461547  [ 8000 /  8784]\n",
            "Accuracy: 0.5368852459016393, Avg loss: 1.6396381855010986 \n",
            "\n",
            "Epoch 2\n",
            "________________________________________\n",
            "loss: 1.670395  [    0 /  8784]\n",
            "loss: 1.822924  [ 4000 /  8784]\n",
            "loss: 0.838009  [ 8000 /  8784]\n",
            "Accuracy: 0.5897085610200364, Avg loss: 1.443270206451416 \n",
            "\n",
            "Epoch 3\n",
            "________________________________________\n",
            "loss: 0.759084  [    0 /  8784]\n",
            "loss: 1.151868  [ 4000 /  8784]\n",
            "loss: 2.312820  [ 8000 /  8784]\n",
            "Accuracy: 0.6020036429872495, Avg loss: 1.3618360757827759 \n",
            "\n",
            "Epoch 4\n",
            "________________________________________\n",
            "loss: 1.303294  [    0 /  8784]\n",
            "loss: 1.555584  [ 4000 /  8784]\n",
            "loss: 1.680857  [ 8000 /  8784]\n",
            "Accuracy: 0.6183970856102003, Avg loss: 1.3246428966522217 \n",
            "\n",
            "Epoch 5\n",
            "________________________________________\n",
            "loss: 1.955585  [    0 /  8784]\n",
            "loss: 1.066343  [ 4000 /  8784]\n",
            "loss: 0.126044  [ 8000 /  8784]\n",
            "Accuracy: 0.6238615664845173, Avg loss: 1.2791104316711426 \n",
            "\n",
            "Epoch 6\n",
            "________________________________________\n",
            "loss: 0.972339  [    0 /  8784]\n",
            "loss: 1.399474  [ 4000 /  8784]\n",
            "loss: 1.435126  [ 8000 /  8784]\n",
            "Accuracy: 0.6493624772313297, Avg loss: 1.2926084995269775 \n",
            "\n",
            "Epoch 7\n",
            "________________________________________\n",
            "loss: 0.727380  [    0 /  8784]\n",
            "loss: 0.979800  [ 4000 /  8784]\n",
            "loss: 0.908495  [ 8000 /  8784]\n",
            "Accuracy: 0.6479963570127505, Avg loss: 1.3052749633789062 \n",
            "\n",
            "Epoch 8\n",
            "________________________________________\n",
            "loss: 0.467474  [    0 /  8784]\n",
            "loss: 0.375987  [ 4000 /  8784]\n",
            "loss: 1.127134  [ 8000 /  8784]\n",
            "Accuracy: 0.6475409836065574, Avg loss: 1.2877639532089233 \n",
            "\n",
            "Epoch 9\n",
            "________________________________________\n",
            "loss: 1.336203  [    0 /  8784]\n",
            "loss: 0.339609  [ 4000 /  8784]\n",
            "loss: 0.319065  [ 8000 /  8784]\n",
            "Accuracy: 0.6520947176684881, Avg loss: 1.285042405128479 \n",
            "\n",
            "Epoch 10\n",
            "________________________________________\n",
            "loss: 1.246154  [    0 /  8784]\n",
            "loss: 2.357864  [ 4000 /  8784]\n",
            "loss: 0.806608  [ 8000 /  8784]\n",
            "Accuracy: 0.6484517304189436, Avg loss: 1.3061941862106323 \n",
            "\n",
            "CPU times: user 31.2 s, sys: 554 ms, total: 31.8 s\n",
            "Wall time: 32 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно увидеть, что точность в среднем равняется 62%. Это не такие хорошие результаты."
      ],
      "metadata": {
        "id": "MVvRxU819wC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kquWp9CBv_5r",
        "outputId": "d6ecd1e0-8d41-47ee-bb68-3e0a15be4990"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f0bb91b78f0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "embeddings_net = SurnamesClassifier(vocab, len(labeler.classes_)).to(DEVICE)\n",
        "loss_fn = nn.NLLLoss()\n",
        "optimizer = optim.Adam(embeddings_net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "Eb7Ppj5cv-Re"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Embedding\n",
        "embeddings_net.use_embedding = True\n",
        "emb = common(\n",
        "    epochs=15,\n",
        "    model=embeddings_net,\n",
        "    loss_fn=loss_fn,\n",
        "    optimizer=optimizer,\n",
        "    train_dataloader=DataLoader(train_indices, batch_size=8, shuffle=True),\n",
        "    test_dataloader=DataLoader(test_indices, batch_size=512),\n",
        "    verbose=500,\n",
        "    device=DEVICE,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QA6v9chwR5y",
        "outputId": "506de5b6-422b-4bc5-ac43-57397cf3cffe"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "________________________________________\n",
            "loss: 2.909279  [    0 /  8784]\n",
            "loss: 1.590480  [ 4000 /  8784]\n",
            "loss: 1.432680  [ 8000 /  8784]\n",
            "Accuracy: 0.5491803278688525, Avg loss: 1.624413251876831 \n",
            "\n",
            "Epoch 2\n",
            "________________________________________\n",
            "loss: 1.999613  [    0 /  8784]\n",
            "loss: 1.635629  [ 4000 /  8784]\n",
            "loss: 1.539364  [ 8000 /  8784]\n",
            "Accuracy: 0.6388888888888888, Avg loss: 1.331519603729248 \n",
            "\n",
            "Epoch 3\n",
            "________________________________________\n",
            "loss: 1.428237  [    0 /  8784]\n",
            "loss: 1.474102  [ 4000 /  8784]\n",
            "loss: 2.516058  [ 8000 /  8784]\n",
            "Accuracy: 0.6425318761384335, Avg loss: 1.2822692394256592 \n",
            "\n",
            "Epoch 4\n",
            "________________________________________\n",
            "loss: 1.341563  [    0 /  8784]\n",
            "loss: 0.821681  [ 4000 /  8784]\n",
            "loss: 0.481285  [ 8000 /  8784]\n",
            "Accuracy: 0.6461748633879781, Avg loss: 1.2427399158477783 \n",
            "\n",
            "Epoch 5\n",
            "________________________________________\n",
            "loss: 1.201291  [    0 /  8784]\n",
            "loss: 1.654966  [ 4000 /  8784]\n",
            "loss: 0.556950  [ 8000 /  8784]\n",
            "Accuracy: 0.6712204007285975, Avg loss: 1.1412407159805298 \n",
            "\n",
            "Epoch 6\n",
            "________________________________________\n",
            "loss: 0.700479  [    0 /  8784]\n",
            "loss: 0.648124  [ 4000 /  8784]\n",
            "loss: 1.146015  [ 8000 /  8784]\n",
            "Accuracy: 0.6844262295081968, Avg loss: 1.0884883403778076 \n",
            "\n",
            "Epoch 7\n",
            "________________________________________\n",
            "loss: 1.251249  [    0 /  8784]\n",
            "loss: 0.841206  [ 4000 /  8784]\n",
            "loss: 0.891386  [ 8000 /  8784]\n",
            "Accuracy: 0.6857923497267759, Avg loss: 1.1233831644058228 \n",
            "\n",
            "Epoch 8\n",
            "________________________________________\n",
            "loss: 0.734153  [    0 /  8784]\n",
            "loss: 1.194348  [ 4000 /  8784]\n",
            "loss: 0.826835  [ 8000 /  8784]\n",
            "Accuracy: 0.6980874316939891, Avg loss: 1.0995844602584839 \n",
            "\n",
            "Epoch 9\n",
            "________________________________________\n",
            "loss: 0.342961  [    0 /  8784]\n",
            "loss: 0.888469  [ 4000 /  8784]\n",
            "loss: 0.613469  [ 8000 /  8784]\n",
            "Accuracy: 0.7112932604735883, Avg loss: 1.0709526538848877 \n",
            "\n",
            "Epoch 10\n",
            "________________________________________\n",
            "loss: 0.233947  [    0 /  8784]\n",
            "loss: 0.148220  [ 4000 /  8784]\n",
            "loss: 0.115430  [ 8000 /  8784]\n",
            "Accuracy: 0.7167577413479053, Avg loss: 1.137039303779602 \n",
            "\n",
            "Epoch 11\n",
            "________________________________________\n",
            "loss: 0.722639  [    0 /  8784]\n",
            "loss: 0.573480  [ 4000 /  8784]\n",
            "loss: 1.028400  [ 8000 /  8784]\n",
            "Accuracy: 0.7062841530054644, Avg loss: 1.1475852727890015 \n",
            "\n",
            "Epoch 12\n",
            "________________________________________\n",
            "loss: 1.043068  [    0 /  8784]\n",
            "loss: 1.326179  [ 4000 /  8784]\n",
            "loss: 0.350149  [ 8000 /  8784]\n",
            "Accuracy: 0.7163023679417122, Avg loss: 1.1587196588516235 \n",
            "\n",
            "Epoch 13\n",
            "________________________________________\n",
            "loss: 1.560237  [    0 /  8784]\n",
            "loss: 0.787294  [ 4000 /  8784]\n",
            "loss: 0.441055  [ 8000 /  8784]\n",
            "Accuracy: 0.7199453551912568, Avg loss: 1.1621586084365845 \n",
            "\n",
            "Epoch 14\n",
            "________________________________________\n",
            "loss: 0.515750  [    0 /  8784]\n",
            "loss: 0.120277  [ 4000 /  8784]\n",
            "loss: 0.707628  [ 8000 /  8784]\n",
            "Accuracy: 0.7126593806921676, Avg loss: 1.2573211193084717 \n",
            "\n",
            "Epoch 15\n",
            "________________________________________\n",
            "loss: 0.261551  [    0 /  8784]\n",
            "loss: 1.405261  [ 4000 /  8784]\n",
            "loss: 0.793321  [ 8000 /  8784]\n",
            "Accuracy: 0.7144808743169399, Avg loss: 1.240553617477417 \n",
            "\n",
            "CPU times: user 49.4 s, sys: 318 ms, total: 49.7 s\n",
            "Wall time: 50.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно заметить, что точность в среднем 70%, что выше, чем в предыдущем эксперименте. Однако скорость выполнения ниже."
      ],
      "metadata": {
        "id": "1Jz_Gfm29i2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: прогнать несколько фамилий студентов группы через модели и проверить результат. Для каждой фамилии выводить 3 наиболее вероятных предсказания."
      ],
      "metadata": {
        "id": "t-f04iRbwmzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_func(dataloader=DataLoader(test_indices, batch_size=512), model=embeddings_net, loss_fn=loss_fn, device=DEVICE);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgMjIxEJwtE5",
        "outputId": "44cf51e1-6a99-4f16-c90d-8ac56e589f75"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7144808743169399, Avg loss: 1.240553617477417 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Измерим точность нашей модели на перечне фамилий студентов в качестве данных."
      ],
      "metadata": {
        "id": "rNtMFCCb-pqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def total_conclusion(surname, target, model, vocab, labeler, n, device):\n",
        "  x = vocab.encode(surname.lower())\n",
        "  x = x.to(device)\n",
        "\n",
        "  pred = model(x.unsqueeze(0))\n",
        "  pred_prob, pred_label_indices = F.softmax(pred, 1).topk(n, dim=1)\n",
        "  pred_labels = labeler.inverse_transform(pred_label_indices.squeeze().cpu())\n",
        "\n",
        "  predict = \", \".join([f\"{label} ({prob})\" for (label, prob) in zip(pred_labels, pred_prob.squeeze())])\n",
        "  print(f'Surname.....{surname}')\n",
        "  print(f'True........{target}')\n",
        "  print(f'Predicts....{predict}\\n')"
      ],
      "metadata": {
        "id": "-l0BqqH2w1qO"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "students_PI19_3 = [\n",
        "  \"Alexandrova\",\n",
        "  \"Baranov\",\n",
        "  \"Brusova\",\n",
        "  \"Volkova\",\n",
        "  \"Gasanova\",\n",
        "  \"Danilin\",\n",
        "  \"Demenchuk\",\n",
        "  \"Egorov\",\n",
        "  \"Popova\",\n",
        "  \"Polikarpova\",\n",
        "  \"Khamikoeva\",\n",
        "]\n",
        "\n",
        "for surname in students_PI19_3:\n",
        "  total_conclusion(surname=surname, target=\"Russian\", model=embeddings_net, vocab=vocab, labeler=labeler, n=3, device=DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8GsWSALxhlj",
        "outputId": "46aac830-e4d3-4796-82ba-27fb3cb025f0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Surname.....Alexandrova\n",
            "True........Russian\n",
            "Predicts....Russian (0.9935316443443298), English (0.004402637481689453), French (0.00096506456611678)\n",
            "\n",
            "Surname.....Baranov\n",
            "True........Russian\n",
            "Predicts....Russian (1.0), Czech (1.0124450648511807e-13), French (5.999957204249479e-14)\n",
            "\n",
            "Surname.....Brusova\n",
            "True........Russian\n",
            "Predicts....Russian (0.7696868777275085), Czech (0.13018909096717834), Japanese (0.03549609333276749)\n",
            "\n",
            "Surname.....Volkova\n",
            "True........Russian\n",
            "Predicts....Russian (0.999427855014801), Czech (0.0005587751511484385), Polish (6.436499916162575e-06)\n",
            "\n",
            "Surname.....Gasanova\n",
            "True........Russian\n",
            "Predicts....Russian (0.999630331993103), Japanese (0.00017946900334209204), Czech (8.325811359100044e-05)\n",
            "\n",
            "Surname.....Danilin\n",
            "True........Russian\n",
            "Predicts....Russian (0.9999924898147583), English (5.488362603500718e-06), French (1.6447306734335143e-06)\n",
            "\n",
            "Surname.....Demenchuk\n",
            "True........Russian\n",
            "Predicts....Russian (0.5304750204086304), German (0.20539936423301697), English (0.13594506680965424)\n",
            "\n",
            "Surname.....Egorov\n",
            "True........Russian\n",
            "Predicts....Russian (0.9999717473983765), English (2.2812857423559763e-05), Czech (2.9528011964430334e-06)\n",
            "\n",
            "Surname.....Popova\n",
            "True........Russian\n",
            "Predicts....Russian (0.6362946629524231), Czech (0.34747517108917236), Spanish (0.01565135456621647)\n",
            "\n",
            "Surname.....Polikarpova\n",
            "True........Russian\n",
            "Predicts....Russian (0.9983882904052734), Czech (0.0007652721833437681), English (0.0004021663044113666)\n",
            "\n",
            "Surname.....Khamikoeva\n",
            "True........Russian\n",
            "Predicts....Russian (1.0), Czech (1.0813389073049962e-10), Japanese (4.542122429042339e-13)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно заметить, что использование embedding увеличивает точность модели. Точность предсказания очень высокая при классификации практически всех фамилий."
      ],
      "metadata": {
        "id": "grwikLaG-nwF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo-hf5CQ0iWv"
      },
      "source": [
        "## 3. Классификация обзоров на фильмы (ConvNet)\n",
        "\n",
        "Датасет: https://disk.yandex.ru/d/tdinpb0nN_Dsrg\n",
        "\n",
        "2.1 Создайте набор данных на основе файлов polarity/positive_reviews.csv (положительные отзывы) и polarity/negative_reviews.csv (отрицательные отзывы). Разбейте на обучающую и тестовую выборку.\n",
        "  * токен = __слово__\n",
        "  * данные для обучения в датасете представляются в виде последовательности индексов токенов\n",
        "  * словарь создается на основе _только_ обучающей выборки. Для корректной обработки ситуаций, когда в тестовой выборке встретится токен, который не хранится в словаре, добавьте в словарь специальный токен `<UNK>`\n",
        "  * добавьте предобработку текста\n",
        "\n",
        "2.2. Обучите классификатор.\n",
        "  \n",
        "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding` \n",
        "    - подберите адекватную размерность вектора эмбеддинга: \n",
        "    - модуль `nn.Embedding` обучается\n",
        "\n",
        "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
        "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`\n",
        "\n",
        "\n",
        "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n",
        "* Целевое значение accuracy на валидации - 70+%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Создайте набор данных на основе файлов polarity/positive_reviews.csv (положительные отзывы) и polarity/negative_reviews.csv (отрицательные отзывы). Разбейте на обучающую и тестовую выборку.\n",
        "  * токен = __слово__\n",
        "  * данные для обучения в датасете представляются в виде последовательности индексов токенов\n",
        "  * словарь создается на основе _только_ обучающей выборки. Для корректной обработки ситуаций, когда в тестовой выборке встретится токен, который не хранится в словаре, добавьте в словарь специальный токен `<UNK>`\n",
        "  * добавьте предобработку текста\n"
      ],
      "metadata": {
        "id": "H_x-RHm7Ab7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "26eBtqGsB8Ff"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos(word): # части речи wordnet и nltk по-разному называются, тут переводится к частям речи wordnet\n",
        "  tag = nltk.pos_tag([word])[0][1]\n",
        "  if tag.startswith('J'):\n",
        "    return wordnet.ADJ\n",
        "  elif tag.startswith('V'):\n",
        "    return wordnet.VERB\n",
        "  elif tag.startswith('R'):\n",
        "    return wordnet.ADV\n",
        "  else:\n",
        "    return wordnet.NOUN\n",
        "\n",
        "#предобработка\n",
        "def preprocess_review(text):\n",
        "  #приводим к нижнему регистру\n",
        "  text = text.lower()\n",
        "  #регулярным выражением удаляем все символы кроме букв латинского алфавита\n",
        "  text = re.sub(r\"[^a-z]\", repl=\" \", string=text, flags=re.MULTILINE)\n",
        "\n",
        "  lemmatizer = nltk.WordNetLemmatizer()\n",
        "  words = []\n",
        "  for word in nltk.word_tokenize(text):\n",
        "    if word not in STOPWORDS:  # отсеиваем стоп-слова перед лемматизацией, чтобы лишнее не приводить\n",
        "      lemma = lemmatizer.lemmatize(word, pos=get_pos(word))\n",
        "      # избавляемся еще от части слов\n",
        "      if lemma not in STOPWORDS and len(lemma) > 2:\n",
        "        words.append(lemma)\n",
        "\n",
        "  return \" \".join(words)"
      ],
      "metadata": {
        "id": "r157JDv1BkCg"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#создаем класс для формирования набора данных\n",
        "class ReviewsDataset(Dataset):\n",
        "\n",
        "  def __init__(self, positive, negative, seed):\n",
        "    self.positive = positive #пути к данным\n",
        "    self.negative = negative\n",
        "    self.positive_reviews = self.read_reviews(positive, preprocess_review) #читает и обрабатывает данные\n",
        "    self.negative_reviews = self.read_reviews(negative, preprocess_review)\n",
        "\n",
        "    data = self.positive_reviews + self.negative_reviews # слияние данных\n",
        "    targets = torch.cat([torch.ones(len(self.positive_reviews)), torch.zeros(len(self.negative_reviews))]) #слияние данных для классов\n",
        "\n",
        "    if seed is not None:\n",
        "      torch.manual_seed(seed)\n",
        "    indices = torch.randperm(len(data)) # генераци индексов в случайном порядке для того, чтобы перемешать данные\n",
        "\n",
        "    self.data = [data[i] for i in indices] # перемешиваются данные \n",
        "    self.targets = targets[indices].to(torch.long) # перемешиваются targets\n",
        "\n",
        "  @staticmethod\n",
        "  def read_reviews(path, process): # для обработки данных перед слиянием\n",
        "    reviews = []\n",
        "    with open(path) as f:\n",
        "      for review in f.readlines():\n",
        "        review = process(review)\n",
        "        if review:\n",
        "          reviews.append(review)\n",
        "    return reviews\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    return self.data[i], self.targets[i]"
      ],
      "metadata": {
        "id": "s120io9xC08m"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_dataset = ReviewsDataset(\n",
        "  '/content/drive/MyDrive/ML_FU/Lab_6_embeddings/data/positive_reviews.txt',\n",
        "  '/content/drive/MyDrive/ML_FU/Lab_6_embeddings/data/negative_reviews.txt',\n",
        "    seed=0\n",
        ")\n",
        "len(reviews_dataset), reviews_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYTgQmkiAaVU",
        "outputId": "3cbf4837-e6e2-4397-d44e-242a8e005fa6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10660, ('play less like come age romance infomercial', tensor(0)))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "#разделим данные на обучающую и тестовую выборки\n",
        "train_reviews, test_reviews = split_train_test(reviews_dataset, train_part=0.8)\n",
        "len(train_reviews), len(test_reviews)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HOVU7PXEz4N",
        "outputId": "3e055427-0de9-463b-8f6a-5d45a5e10ee6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8528, 2132)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReviewsVocab:\n",
        "  pad = \"<PAD>\"\n",
        "  unknown = \"<UNK>\"\n",
        "\n",
        "  def __init__(self, reviews):\n",
        "    unique_list = set()\n",
        "    max_seq_len = 0\n",
        "    for review in reviews:\n",
        "      words = nltk.word_tokenize(review)\n",
        "      unique_list.update(words)\n",
        "      max_seq_len = max(len(words), max_seq_len)\n",
        "\n",
        "    self.symbols = [self.pad, self.unknown, *unique_list]\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "    word_ind = {word: i for i, word in enumerate(self.symbols)}\n",
        "    # 1 - индекс служебного символа для отсутствующего ключа\n",
        "    self.word_ind = defaultdict(lambda: 1, word_ind)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.symbols)\n",
        "\n",
        "  #Как и в прошлый раз функции кодирования и декодирования\n",
        "  \n",
        "  @lru_cache(maxsize=8192) \n",
        "  def encode(self, review):\n",
        "    indices = [self.word_ind[word] for word in nltk.word_tokenize(review)]\n",
        "    indices += [self.word_ind[self.pad]] * (self.max_seq_len - len(indices))\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "  def decode(self, indices):\n",
        "    pad_indices = torch.nonzero(indices == self.word_ind[self.pad], as_tuple=True)[0]\n",
        "    if len(pad_indices):\n",
        "      indices = indices[:pad_indices[0]] #без служебных символов\n",
        "    return \" \".join(self.symbols[i] for i in indices)"
      ],
      "metadata": {
        "id": "Hy0-xu9yFIFx"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = ReviewsVocab([review for review, _ in train_reviews])\n",
        "print(f'Symbols: {len(vocab)}, Max len: {vocab.max_seq_len}')\n",
        "encoded = vocab.encode('this is shy review')\n",
        "encoded, vocab.decode(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U-ZYTniHNWg",
        "outputId": "6234a481-bff1-4099-ea14-0bc04584f8b1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbols: 13229, Max len: 29\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([   1,    1, 9836,  424,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0]), '<UNK> <UNK> shy review')"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2. Обучите классификатор.\n",
        "  \n",
        "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding` \n",
        "    - подберите адекватную размерность вектора эмбеддинга: \n",
        "    - модуль `nn.Embedding` обучается\n",
        "\n",
        "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
        "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`"
      ],
      "metadata": {
        "id": "vt4QkDkPAuzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReviewsClassifier(nn.Module):\n",
        "  last_out_channals = 64\n",
        "  adaptive_avg_pool = 8\n",
        "\n",
        "  def __init__(self, num_embeddings, embedding_dim):\n",
        "    super(ReviewsClassifier, self).__init__()\n",
        "\n",
        "    #строим модель\n",
        "    self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n",
        "    self.features = nn.Sequential(\n",
        "      nn.Conv1d(in_channels=embedding_dim, out_channels=self.last_out_channals, kernel_size=2), # сверточный слой, embedding_dim - задается любая размерность\n",
        "      nn.BatchNorm1d(num_features=self.last_out_channals), # для улучшения работы модели\n",
        "      nn.ReLU(), # функция активации\n",
        "      nn.MaxPool1d(kernel_size=2), # уменьшение размерности в 2 раза\n",
        "    )\n",
        "    self.avgpool = nn.AdaptiveAvgPool1d(self.adaptive_avg_pool) # приведение к нужной размерности\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Linear(self.last_out_channals * self.adaptive_avg_pool, 256),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(),\n",
        "      nn.Linear(256, 2),\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    x = self.embedding(x) # приводим данные к значениям эмбеддинга (индексы слов в признаки)\n",
        "    x = x.reshape(x.size(0), x.size(2), x.size(1)) # меняем последнюю и предпоследнюю размерности\n",
        "    x = self.features(x) # сверточный слой\n",
        "    x = self.avgpool(x) # приведение к нужной размерности\n",
        "    x = torch.flatten(x, 1) # размерность, без которой не сработает линейный слой\n",
        "    x = self.classifier(x) # линейный слой\n",
        "    return x\n",
        "\n",
        "\n",
        "def collate(batch): # функция между даталоадером и формированием батча для кодирования данных в батче\n",
        "  x_s, y_s = [], []\n",
        "  for x, y in batch:\n",
        "    x_s.append(vocab.encode(x))\n",
        "    y_s.append(y)\n",
        "  return torch.vstack(x_s), torch.hstack(y_s)"
      ],
      "metadata": {
        "id": "ubsyr3oOA0Od"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "net = ReviewsClassifier(num_embeddings=len(vocab), embedding_dim=128).to(DEVICE)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.00091)\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau( # для формирование lr\n",
        "    optimizer=optimizer,\n",
        "    mode=\"min\", # стремимся к минимизации параметра\n",
        "    patience=5, # каждые 5 эпох\n",
        "    factor=0.33, # насколько умножается изначальный lr по прошествии 5 эпох, если ошибка не снизилась больше чем на threshold\n",
        "    min_lr=0.000001,\n",
        "    threshold=0.001,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "DuUXEa_qL2Ya"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_reviews, batch_size=22, collate_fn=collate, shuffle=True) # shuffle - на каждую новую эпоху данные в разном порядке\n",
        "test_dataloader = DataLoader(test_reviews, batch_size=512, collate_fn=collate)"
      ],
      "metadata": {
        "id": "TFK6CklNMC_6"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "_ = common(\n",
        "    epochs=20,\n",
        "    model=net,\n",
        "    loss_fn=loss_fn,\n",
        "    optimizer=optimizer,\n",
        "    train_dataloader=train_dataloader,\n",
        "    test_dataloader=test_dataloader,\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    verbose=150,\n",
        "    device=DEVICE,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk-7tLqBMJPO",
        "outputId": "967cf3c8-c3a4-463f-8be6-86fcc804e5e9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "________________________________________\n",
            "loss: 0.659027  [    0 /  8528]\n",
            "loss: 0.713437  [ 3300 /  8528]\n",
            "loss: 0.700269  [ 6600 /  8528]\n",
            "Accuracy: 0.4910881801125704, Avg loss: 0.6932711005210876 \n",
            "\n",
            "Epoch 2\n",
            "________________________________________\n",
            "loss: 0.690713  [    0 /  8528]\n",
            "loss: 0.697010  [ 3300 /  8528]\n",
            "loss: 0.699278  [ 6600 /  8528]\n",
            "Accuracy: 0.4915572232645403, Avg loss: 0.6936709880828857 \n",
            "\n",
            "Epoch 3\n",
            "________________________________________\n",
            "loss: 0.688256  [    0 /  8528]\n",
            "loss: 0.690769  [ 3300 /  8528]\n",
            "loss: 0.681394  [ 6600 /  8528]\n",
            "Accuracy: 0.49906191369606, Avg loss: 0.6930369138717651 \n",
            "\n",
            "Epoch 4\n",
            "________________________________________\n",
            "loss: 0.698211  [    0 /  8528]\n",
            "loss: 0.729387  [ 3300 /  8528]\n",
            "loss: 0.703668  [ 6600 /  8528]\n",
            "Accuracy: 0.550187617260788, Avg loss: 0.6871483325958252 \n",
            "\n",
            "Epoch 5\n",
            "________________________________________\n",
            "loss: 0.658416  [    0 /  8528]\n",
            "loss: 0.655683  [ 3300 /  8528]\n",
            "loss: 0.612282  [ 6600 /  8528]\n",
            "Accuracy: 0.6027204502814258, Avg loss: 0.6584477424621582 \n",
            "\n",
            "Epoch 6\n",
            "________________________________________\n",
            "loss: 0.518503  [    0 /  8528]\n",
            "loss: 0.401565  [ 3300 /  8528]\n",
            "loss: 0.653048  [ 6600 /  8528]\n",
            "Accuracy: 0.6074108818011257, Avg loss: 0.7229486703872681 \n",
            "\n",
            "Epoch 7\n",
            "________________________________________\n",
            "loss: 0.339923  [    0 /  8528]\n",
            "loss: 0.312620  [ 3300 /  8528]\n",
            "loss: 0.289302  [ 6600 /  8528]\n",
            "Accuracy: 0.6622889305816135, Avg loss: 0.6727651357650757 \n",
            "\n",
            "Epoch 8\n",
            "________________________________________\n",
            "loss: 0.346475  [    0 /  8528]\n",
            "loss: 0.590774  [ 3300 /  8528]\n",
            "loss: 0.243189  [ 6600 /  8528]\n",
            "Accuracy: 0.6660412757973734, Avg loss: 0.8246471285820007 \n",
            "\n",
            "Epoch 9\n",
            "________________________________________\n",
            "loss: 0.121024  [    0 /  8528]\n",
            "loss: 0.118647  [ 3300 /  8528]\n",
            "loss: 0.099406  [ 6600 /  8528]\n",
            "Accuracy: 0.6712007504690432, Avg loss: 0.8755964040756226 \n",
            "\n",
            "Epoch 10\n",
            "________________________________________\n",
            "loss: 0.284821  [    0 /  8528]\n",
            "loss: 0.091622  [ 3300 /  8528]\n",
            "loss: 0.086613  [ 6600 /  8528]\n",
            "Accuracy: 0.6674484052532833, Avg loss: 1.1997147798538208 \n",
            "\n",
            "Epoch 11\n",
            "________________________________________\n",
            "loss: 0.089036  [    0 /  8528]\n",
            "loss: 0.111419  [ 3300 /  8528]\n",
            "loss: 0.014449  [ 6600 /  8528]\n",
            "Accuracy: 0.6763602251407129, Avg loss: 1.3767575025558472 \n",
            "\n",
            "Epoch 00011: reducing learning rate of group 0 to 3.0030e-04.\n",
            "Epoch 12\n",
            "________________________________________\n",
            "loss: 0.012707  [    0 /  8528]\n",
            "loss: 0.037376  [ 3300 /  8528]\n",
            "loss: 0.042951  [ 6600 /  8528]\n",
            "Accuracy: 0.6787054409005628, Avg loss: 1.5097700357437134 \n",
            "\n",
            "Epoch 13\n",
            "________________________________________\n",
            "loss: 0.073641  [    0 /  8528]\n",
            "loss: 0.046234  [ 3300 /  8528]\n",
            "loss: 0.049309  [ 6600 /  8528]\n",
            "Accuracy: 0.6791744840525328, Avg loss: 1.6267776489257812 \n",
            "\n",
            "Epoch 14\n",
            "________________________________________\n",
            "loss: 0.075103  [    0 /  8528]\n",
            "loss: 0.030226  [ 3300 /  8528]\n",
            "loss: 0.017942  [ 6600 /  8528]\n",
            "Accuracy: 0.6716697936210131, Avg loss: 1.7919111251831055 \n",
            "\n",
            "Epoch 15\n",
            "________________________________________\n",
            "loss: 0.033764  [    0 /  8528]\n",
            "loss: 0.005955  [ 3300 /  8528]\n",
            "loss: 0.120631  [ 6600 /  8528]\n",
            "Accuracy: 0.6726078799249531, Avg loss: 1.8961350917816162 \n",
            "\n",
            "Epoch 16\n",
            "________________________________________\n",
            "loss: 0.005666  [    0 /  8528]\n",
            "loss: 0.007410  [ 3300 /  8528]\n",
            "loss: 0.087653  [ 6600 /  8528]\n",
            "Accuracy: 0.6843339587242027, Avg loss: 2.021371364593506 \n",
            "\n",
            "Epoch 17\n",
            "________________________________________\n",
            "loss: 0.002194  [    0 /  8528]\n",
            "loss: 0.007759  [ 3300 /  8528]\n",
            "loss: 0.039796  [ 6600 /  8528]\n",
            "Accuracy: 0.6824577861163227, Avg loss: 2.0606725215911865 \n",
            "\n",
            "Epoch 00017: reducing learning rate of group 0 to 9.9099e-05.\n",
            "Epoch 18\n",
            "________________________________________\n",
            "loss: 0.038880  [    0 /  8528]\n",
            "loss: 0.044875  [ 3300 /  8528]\n",
            "loss: 0.006196  [ 6600 /  8528]\n",
            "Accuracy: 0.6838649155722326, Avg loss: 2.1115870475769043 \n",
            "\n",
            "Epoch 19\n",
            "________________________________________\n",
            "loss: 0.000219  [    0 /  8528]\n",
            "loss: 0.005359  [ 3300 /  8528]\n",
            "loss: 0.013252  [ 6600 /  8528]\n",
            "Accuracy: 0.6815196998123827, Avg loss: 2.1763436794281006 \n",
            "\n",
            "Epoch 20\n",
            "________________________________________\n",
            "loss: 0.071501  [    0 /  8528]\n",
            "loss: 0.000705  [ 3300 /  8528]\n",
            "loss: 0.004740  [ 6600 /  8528]\n",
            "Accuracy: 0.6801125703564728, Avg loss: 2.2171339988708496 \n",
            "\n",
            "CPU times: user 1min 40s, sys: 1.6 s, total: 1min 41s\n",
            "Wall time: 1min 41s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "batch_size важный параметр, потому что если неверно его подобрать, мы застреваем в неудачном состоянии модели."
      ],
      "metadata": {
        "id": "TZCS5bAcMVjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "С увеличением эпохи тествая ошибка начинает очень сильно расти. Это явная проблема модели."
      ],
      "metadata": {
        "id": "2xIosECaM2_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n",
        "* Целевое значение accuracy на валидации - 70+%"
      ],
      "metadata": {
        "id": "ZcsxQnrkA0tB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad() # не считать градиенты \n",
        "def get_y_test_y_pred(model, test_dataloader, device):\n",
        "    \n",
        "  model.eval()\n",
        "  y_test = []\n",
        "  y_pred = []\n",
        "  for x, y in test_dataloader:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    pred = model(x).argmax(1)\n",
        "    y_test.append(y)\n",
        "    y_pred.append(pred)\n",
        "\n",
        "    del x\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  return torch.hstack(y_test).detach().cpu(), torch.hstack(y_pred).detach().cpu()"
      ],
      "metadata": {
        "id": "7wT2nEt3NZ3g"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test, y_pred = get_y_test_y_pred(net, test_dataloader, DEVICE)\n",
        "\n",
        "print(metrics.classification_report(\n",
        "  y_true=y_test,\n",
        "  y_pred=y_pred,\n",
        "  target_names=[\"negative\", \"positive\"],\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Hy6mWPgA3Qf",
        "outputId": "0ef7a936-ca43-4ac7-dcea-3cbad8d5dec2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.68      0.67      0.67      1061\n",
            "    positive       0.68      0.69      0.69      1071\n",
            "\n",
            "    accuracy                           0.68      2132\n",
            "   macro avg       0.68      0.68      0.68      2132\n",
            "weighted avg       0.68      0.68      0.68      2132\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Наблюдаем результаты работы модели. \n",
        "\n",
        "Все метрики показывают +-68%.\n",
        "Таким образом по отчету метрик классификации можно сделать вывод, что особой несбалансированности между метриками нет, поэтому accuracy тоже можно верить.\n",
        "Точность = 68%.\n"
      ],
      "metadata": {
        "id": "titTAJ7UNtIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def total_conclusion(review, target, model, vocab, target_names, device):\n",
        "  x = vocab.encode(preprocess_review(review))\n",
        "  x = x.to(device)\n",
        "  pred = model(x.unsqueeze(0))\n",
        "  pred_prob, pred_label_idx = F.softmax(pred, 1).max(dim=1)\n",
        "  pred_label = target_names[pred_label_idx.cpu()]\n",
        "\n",
        "  print(f'Review : {review}')\n",
        "  print(f'True   : {target}')\n",
        "  print(f'Predict: {pred_label} ({pred_prob.item()})\\n')"
      ],
      "metadata": {
        "id": "PtlQm5QoNnu8"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = [\n",
        "  ('A terrible plot. I never seen anything so bad.', 'negative'),\n",
        "  ('A fascinating story. I think the music is just beautiful.', 'positive'),\n",
        "]\n",
        "for review, target in reviews:\n",
        "  total_conclusion(review=review, target=target, model=net, vocab=vocab, target_names=[\"negative\", \"positive\"], device=DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gQmim6XR-sR",
        "outputId": "d1e4058a-49d3-4b58-d86d-5a808116bc1b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review : A terrible plot. I never seen anything so bad.\n",
            "True   : negative\n",
            "Predict: negative (0.9999692440032959)\n",
            "\n",
            "Review : A fascinating story. I think the music is just beautiful.\n",
            "True   : positive\n",
            "Predict: positive (0.999440610408783)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель показывает прекрасные результаты."
      ],
      "metadata": {
        "id": "QCb3ykP4PQPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Как итог:**\n",
        "\n",
        "Батч -  важный параметр, от которого зависит, в каком оптимуме будет находиться модель. (если не то выбрать, то вообще может точность полететь)\n",
        "\n",
        "Чем больше слов мы убираем из данных, тем меньше шума у нас получается. Сюда относятся и стопслова, и слишком короткая лемма."
      ],
      "metadata": {
        "id": "lhXqfDvmPbe8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AouOSFT2AKx7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}