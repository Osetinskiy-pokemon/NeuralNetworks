{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zKMq7dp2W15Y"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import typing as t\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn import metrics\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, random_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMoQc6zIHIwd",
        "outputId": "d4d4df41-c029-43ef-b23b-25f89b262e7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBkqaK5bawXN",
        "outputId": "c61b4bb4-1ea3-4d46-fb62-d772dbac62a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmWCBWxrBUB3"
      },
      "source": [
        "## 1. Генерирование русских имен при помощи RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "990obDBwCC7V"
      },
      "source": [
        "Датасет: https://disk.yandex.ru/i/2yt18jHUgVEoIw\n",
        "\n",
        "1.1 На основе файла name_rus.txt создайте датасет.\n",
        "  * Учтите, что имена могут иметь различную длину\n",
        "  * Добавьте 4 специальных токена: \n",
        "    * `<PAD>` для дополнения последовательности до нужной длины;\n",
        "    * `<UNK>` для корректной обработки ранее не встречавшихся токенов;\n",
        "    * `<SOS>` для обозначения начала последовательности;\n",
        "    * `<EOS>` для обозначения конца последовательности.\n",
        "  * Преобразовывайте строку в последовательность индексов с учетом следующих замечаний:\n",
        "    * в начало последовательности добавьте токен `<SOS>`;\n",
        "    * в конец последовательности добавьте токен `<EOS>` и, при необходимости, несколько токенов `<PAD>`;\n",
        "  * `Dataset.__get_item__` возращает две последовательности: последовательность для обучения и правильный ответ. \n",
        "  \n",
        "  Пример:\n",
        "  ```\n",
        "  s = 'The cat sat on the mat'\n",
        "  # преобразуем в индексы\n",
        "  s_idx = [2, 5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
        "  # получаем x и y (__getitem__)\n",
        "  x = [2, 5, 1, 2, 8, 4, 7, 3, 0]\n",
        "  y = [5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
        "  ```\n",
        "\n",
        "1.2 Создайте и обучите модель для генерации фамилии.\n",
        "\n",
        "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`;\n",
        "  * Используйте рекуррентные слои;\n",
        "  * Задача ставится как предсказание следующего токена в каждом примере из пакета для каждого момента времени. Т.е. в данный момент времени по текущей подстроке предсказывает следующий символ для данной строки (задача классификации);\n",
        "  * Примерная схема реализации метода `forward`:\n",
        "  ```\n",
        "    input_X: [batch_size x seq_len] -> nn.Embedding -> emb_X: [batch_size x seq_len x embedding_size]\n",
        "    emb_X: [batch_size x seq_len x embedding_size] -> nn.RNN -> output: [batch_size x seq_len x hidden_size] \n",
        "    output: [batch_size x seq_len x hidden_size] -> torch.Tensor.reshape -> output: [batch_size * seq_len x hidden_size]\n",
        "    output: [batch_size * seq_len x hidden_size] -> nn.Linear -> output: [batch_size * seq_len x vocab_size]\n",
        "  ```\n",
        "\n",
        "1.3 Напишите функцию, которая генерирует фамилию при помощи обученной модели:\n",
        "  * Построение начинается с последовательности единичной длины, состоящей из индекса токена `<SOS>`;\n",
        "  * Начальное скрытое состояние RNN `h_t = None`;\n",
        "  * В результате прогона последнего токена из построенной последовательности через модель получаете новое скрытое состояние `h_t` и распределение над всеми токенами из словаря;\n",
        "  * Выбираете 1 токен пропорционально вероятности и добавляете его в последовательность (можно воспользоваться `torch.multinomial`);\n",
        "  * Повторяете эти действия до тех пор, пока не сгенерирован токен `<EOS>` или не превышена максимальная длина последовательности.\n",
        "\n",
        "При обучении каждые `k` эпох генерируйте несколько фамилий и выводите их на экран."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 На основе файла name_rus.txt создайте датасет.\n",
        "  * Учтите, что имена могут иметь различную длину\n",
        "  * Добавьте 4 специальных токена: \n",
        "    * `<PAD>` для дополнения последовательности до нужной длины;\n",
        "    * `<UNK>` для корректной обработки ранее не встречавшихся токенов;\n",
        "    * `<SOS>` для обозначения начала последовательности;\n",
        "    * `<EOS>` для обозначения конца последовательности.\n",
        "  * Преобразовывайте строку в последовательность индексов с учетом следующих замечаний:\n",
        "    * в начало последовательности добавьте токен `<SOS>`;\n",
        "    * в конец последовательности добавьте токен `<EOS>` и, при необходимости, несколько токенов `<PAD>`;\n",
        "  * `Dataset.__get_item__` возращает две последовательности: последовательность для обучения и правильный ответ. \n",
        "  \n",
        "  Пример:\n",
        "  ```\n",
        "  s = 'The cat sat on the mat'\n",
        "  # преобразуем в индексы\n",
        "  s_idx = [2, 5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
        "  # получаем x и y (__getitem__)\n",
        "  x = [2, 5, 1, 2, 8, 4, 7, 3, 0]\n",
        "  y = [5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
        "  ```"
      ],
      "metadata": {
        "id": "01iIwwDNJt2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Наша цель - предсказание последующей буквы"
      ],
      "metadata": {
        "id": "mzngzXcNJ7kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "  PAD = \"<PAD>\" # сразу определим несколько токенов\n",
        "  PAD_IDX = 0 # для дополнения последовательности до нужной длины\n",
        "  UNK = \"<UNK>\" # для токенов, которые раньше не встречались\n",
        "  UNK_IDX = 1\n",
        "  SOS = \"<SOS>\" # для определения начала последовательности\n",
        "  SOS_IDX = 2\n",
        "  EOS = \"<EOS>\" # для определения конца последовательности\n",
        "  EOS_IDX = 3\n",
        "\n",
        "  def __init__(self, names):\n",
        "    unique_list = set() # для перечня уникальных элементов\n",
        "    max_seq_len = 0 # для определения максимальной длины последовательности\n",
        "    for name in map(str.lower, names): #наполняем список уникальных элементов\n",
        "      unique_list.update(name)\n",
        "      max_seq_len = max(len(name), max_seq_len) # не забываем искать максимальную длину\n",
        "\n",
        "    self.symbols = [self.PAD, self.UNK, self.SOS, self.EOS, *unique_list] # начало дополняем нашими токенами\n",
        "    self.max_seq_len = max_seq_len + 2  # сохраняем место для <SOS> и <EOS>\n",
        "\n",
        "    symb_ind = {s: i for i, s in enumerate(self.symbols)} # Элемент - индекс\n",
        "    self.symb_ind = defaultdict(lambda: self.UNK_IDX, symb_ind)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.symbols)\n",
        "\n",
        "  # кодирование\n",
        "  def encode(self, name, shift: bool = False):\n",
        "    name = [*name, self.EOS]\n",
        "    if not shift:\n",
        "      name = [self.SOS, *name]\n",
        "    indices = [self.symb_ind[s] for s in name]\n",
        "    indices += [self.PAD_IDX] * (self.max_seq_len - len(indices))\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "  # декодирование\n",
        "  def decode(self, indices: torch.Tensor) -> str:\n",
        "    pad_indices = torch.nonzero(indices == self.ch2i[self.PAD], as_tuple=True)[0]\n",
        "    if len(pad_indices):\n",
        "      indices = indices[:pad_indices[0]]\n",
        "    return \"\".join(self.symbols[i] for i in indices)"
      ],
      "metadata": {
        "id": "Hr8dnvSqJq4q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NDataset:\n",
        "  names: t.List[str]\n",
        "  vocab: Vocab\n",
        "  data: torch.Tensor\n",
        "  targets: torch.Tensor\n",
        "  def __init__(self, path):\n",
        "    self.names = self.get_names(path)\n",
        "    self.vocab = Vocab(self.names)\n",
        "    self.data = torch.vstack([self.encode(name, shift=False) for name in self.names])\n",
        "    self.targets = torch.vstack([self.encode(name, shift=True) for name in self.names])\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.data.size(0)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx], self.targets[idx]\n",
        "\n",
        "  @staticmethod\n",
        "  def get_names(path):\n",
        "    with open(path, encoding=\"cp1251\") as f:\n",
        "      return list(map(lambda s: s.strip().lower(), f))\n",
        "\n",
        "  def encode(self, name, shift: bool = False):\n",
        "    return self.vocab.encode(name, shift=shift)\n",
        "\n",
        "  def decode(self, vector: torch.Tensor):\n",
        "    return self.vocab.decode(vector)"
      ],
      "metadata": {
        "id": "zOT0bk6qRyG5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndataset = NDataset(\"/content/drive/MyDrive/ML_FU/Lab_8/data/name_rus.txt\")\n",
        "print(f\"Количество: {len(ndataset)}\")\n",
        "(ndataset.names[0], *ndataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P53PuBpqUKWF",
        "outputId": "e86d362a-64bd-4a45-f429-82687b129ebf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество: 1988\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('авдокея',\n",
              " tensor([ 2, 20, 30,  4, 23, 31, 32, 10,  3,  0,  0,  0,  0,  0,  0]),\n",
              " tensor([20, 30,  4, 23, 31, 32, 10,  3,  0,  0,  0,  0,  0,  0,  0]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# для разделения выборки на обучающую и тестовую\n",
        "def split_train_test(dataset, train_part):\n",
        "  train_size = round(train_part * len(dataset))\n",
        "  test_size = len(dataset) - train_size\n",
        "  train, test = random_split(dataset, lengths=(train_size, test_size))\n",
        "  return train, test"
      ],
      "metadata": {
        "id": "qE1AF-NNV7tJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# делим выборку\n",
        "torch.manual_seed(0)\n",
        "\n",
        "train_ndataset, test_ndataset = split_train_test(ndataset, train_part=0.8)\n",
        "print(f'Длина обучающей выборки: {len(train_ndataset)} \\nДлина тестовой выборки: {len(test_ndataset)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6SNJmamV0Ag",
        "outputId": "d3d621b6-cbc2-40cd-cf13-186c52a7be84"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Длина обучающей выборки: 1590 \n",
            "Длина тестовой выборки: 398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 Создайте и обучите модель для генерации фамилии.\n",
        "\n",
        "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`;\n",
        "  * Используйте рекуррентные слои;\n",
        "  * Задача ставится как предсказание следующего токена в каждом примере из пакета для каждого момента времени. Т.е. в данный момент времени по текущей подстроке предсказывает следующий символ для данной строки (задача классификации);\n",
        "  * Примерная схема реализации метода `forward`:\n",
        "  ```\n",
        "    input_X: [batch_size x seq_len] -> nn.Embedding -> emb_X: [batch_size x seq_len x embedding_size]\n",
        "    emb_X: [batch_size x seq_len x embedding_size] -> nn.RNN -> output: [batch_size x seq_len x hidden_size] \n",
        "    output: [batch_size x seq_len x hidden_size] -> torch.Tensor.reshape -> output: [batch_size * seq_len x hidden_size]\n",
        "    output: [batch_size * seq_len x hidden_size] -> nn.Linear -> output: [batch_size * seq_len x vocab_size]\n",
        "  ```\n"
      ],
      "metadata": {
        "id": "FQujA9KUJyk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3 Напишите функцию, которая генерирует фамилию при помощи обученной модели:\n",
        "  * Построение начинается с последовательности единичной длины, состоящей из индекса токена `<SOS>`;\n",
        "  * Начальное скрытое состояние RNN `h_t = None`;\n",
        "  * В результате прогона последнего токена из построенной последовательности через модель получаете новое скрытое состояние `h_t` и распределение над всеми токенами из словаря;\n",
        "  * Выбираете 1 токен пропорционально вероятности и добавляете его в последовательность (можно воспользоваться `torch.multinomial`);\n",
        "  * Повторяете эти действия до тех пор, пока не сгенерирован токен `<EOS>` или не превышена максимальная длина последовательности.\n",
        "\n",
        "При обучении каждые `k` эпох генерируйте несколько фамилий и выводите их на экран."
      ],
      "metadata": {
        "id": "Af2nz26QJzW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NamesRNNGenerator(nn.Module):\n",
        "  _STATE = t.Union[t.Optional[torch.Tensor], t.Optional[t.Tuple[torch.Tensor, torch.Tensor]]]\n",
        "  rnn_state: _STATE\n",
        "  def __init__(self, num_embeddings, embedding_dim, rnn_hidden_size, rnn_cls):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=0) # эмбеддинг\n",
        "    self.rnn = rnn_cls(input_size=embedding_dim, hidden_size=rnn_hidden_size, batch_first=True) \n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(rnn_hidden_size, 256), # линейный слой\n",
        "        nn.ReLU(), # функция активации\n",
        "        nn.Dropout(), # с шансом 50% удаляет нейрон (переводит в 0)\n",
        "        nn.Linear(256, num_embeddings), # линейный слой\n",
        "    )\n",
        "    self.reset_rnn_state()\n",
        "\n",
        "  # сбрасываем скрытое состояние\n",
        "  def reset_rnn_state(self):\n",
        "    self.rnn_state = None\n",
        "\n",
        "  # обработка скрытого состояния nn.LSTM\n",
        "  def keep_rnn_state(self, state: _STATE):\n",
        "    if isinstance(self.rnn, nn.LSTM):  \n",
        "      self.rnn_state = (state[0].detach(), state[1].detach())\n",
        "    else:\n",
        "      self.rnn_state = state.detach()\n",
        "\n",
        "  # делаем шаг, где над скрытым состояние внутри сети полный контроль\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    x, rnn_state = self.rnn(x, self.rnn_state)\n",
        "    self.keep_rnn_state(rnn_state)\n",
        "    x = self.fc(x)\n",
        "    return x.permute(0, 2, 1)\n",
        "    \n",
        "  # при смене состояния сети сброс скрытого состояния \n",
        "  def train(self, mode: bool = True):\n",
        "    self.reset_rnn_state()\n",
        "    return super().train(mode)"
      ],
      "metadata": {
        "id": "0QerktBTXO9_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# расчет реальной вероятности\n",
        "def true_prob(pred):\n",
        "  pred = pred - pred.min()\n",
        "  return pred / pred.sum()"
      ],
      "metadata": {
        "id": "1xpSmvn_cJZD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_prob(pred):\n",
        "  return torch.softmax(pred, 0)"
      ],
      "metadata": {
        "id": "N54b70pYcQ6c"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# предсказание каждого следующего после последнего символа имени\n",
        "def name_generate(model, dataset, prompt: str = None, prob: t.Callable[[torch.Tensor], torch.Tensor] = None, device: str = \"cpu\"):\n",
        "  vocab = dataset.vocab\n",
        "  name_vec = [vocab.SOS_IDX]\n",
        "\n",
        "  if prompt:\n",
        "    name_vec += [vocab.symb_ind[s] for s in prompt]\n",
        "  model.eval()  # сбрасываем скрытое состояние\n",
        "\n",
        "  # расчет скрытого состояния для prompt\n",
        "  for i in range(len(name_vec) - 1):\n",
        "    x = torch.tensor([[name_vec[i]]], device=device)\n",
        "    model(x) # изменение скрытого состояния внутри сети\n",
        "  \n",
        "  # предсказание\n",
        "  for i in range(vocab.max_seq_len - len(name_vec) - 2):\n",
        "    x = torch.tensor([[name_vec[-1]]], device=device)\n",
        "    pred = model(x).squeeze() # одиночный батч \n",
        "    if prob: # случайный результат\n",
        "      next_s_idx = torch.multinomial(prob(pred), 1)\n",
        "    else: # лучший результат\n",
        "      next_s_idx = pred.argmax()\n",
        "    if next_s_idx == vocab.EOS_IDX:\n",
        "      break\n",
        "    name_vec.append(next_s_idx.item())\n",
        "  \n",
        "  return \"\".join(vocab.symbols[i] for i in name_vec[1:])"
      ],
      "metadata": {
        "id": "L8WMB83icTBc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  DEVICE = \"cuda\"\n",
        "else:\n",
        "  DEVICE = \"cpu\""
      ],
      "metadata": {
        "id": "ia-ymKiAfAde"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def name_generate_at_end_of_epoch(model: NamesRNNGenerator, dataset: NDataset):\n",
        "  def _on_epoch_end() -> None:\n",
        "    # берется лучший результат\n",
        "    const = name_generate(model, dataset, device=DEVICE)\n",
        "    \n",
        "    # берется случайный результат\n",
        "    true_random = name_generate(model, dataset, prob=true_prob, device=DEVICE)\n",
        "    \n",
        "    # случайный результат на softmax преобразования\n",
        "    softmax_random = name_generate(model, dataset, prob=softmax_prob, device=DEVICE)\n",
        "    \n",
        "    print(f\"Имена: {const} (max), {true_random} (prob), {softmax_random} (softmax)\")\n",
        "  return _on_epoch_end"
      ],
      "metadata": {
        "id": "wQ0kooC-cXWe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "names_gen_net = NamesRNNGenerator(\n",
        "  num_embeddings=len(ndataset.vocab),\n",
        "  embedding_dim=8,\n",
        "  rnn_hidden_size=64,\n",
        "  rnn_cls=nn.RNN,\n",
        ").to(DEVICE)\n",
        "loss_fn = nn.CrossEntropyLoss() # функция ошибки\n",
        "optimizer = optim.Adam(names_gen_net.parameters(), lr=0.001) # оптимизатор, обновляет веса моделей на основании ошибки\n",
        "\n",
        "train_dataloader = DataLoader(train_ndataset, batch_size=32, shuffle=True, drop_last=True) # передавать данные размером с batch_size=32\n",
        "test_dataloader = DataLoader(test_ndataset, batch_size=128, drop_last=True) # передавать данные размером с batch_size=128"
      ],
      "metadata": {
        "id": "l8wqhDWCfMV4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def on_cuda(device: str):\n",
        "    return device == \"cuda\"\n",
        "\n",
        "def train_func(dataloader, model, loss_fn, optimizer, verbose, device):\n",
        "  \n",
        "  model.train() # задаем модели состояние \"будем обучать\"\n",
        "  size = len(dataloader.dataset) # всего сколько данных в датасете\n",
        "  num_batches = len(dataloader) # кол-во батчей в датасете\n",
        "  avg_loss = 0 # для среднего значения ошибки\n",
        "  \n",
        "  for batch, (x, y) in enumerate(dataloader): # проходимся по данным\n",
        "    x, y = x.to(device), y.to(device)  # отправляем данные на устройство (где будут храниться)\n",
        "    \n",
        "    pred = model(x) # вызываем forward\n",
        "    loss = loss_fn(pred, y) # считаем ошибку\n",
        "    \n",
        "    optimizer.zero_grad() # сбрасываем состояние оптимизатора\n",
        "    loss.backward() # обратное распространение ошибки\n",
        "    optimizer.step() # устанавливает новые веса\n",
        "    \n",
        "    avg_loss += loss # суммируем ошибки на протяжении шага обучения\n",
        "    if batch % verbose == 0: # verbose - для вывода данных\n",
        "      print(f\"loss: {loss:>7f}  [{batch * len(x):>5d} / {size:>5d}]\")\n",
        "    \n",
        "    del x, y, pred, loss # чистим память\n",
        "    torch.cuda.empty_cache() # чистим память на gpu\n",
        "  \n",
        "  return avg_loss / num_batches  # возвращаем итоговое значение ошибки шага обучения\n",
        "\n",
        "\n",
        "@torch.no_grad() # не считать градиенты для экономии памяти и времени\n",
        "def test_func(dataloader, model, loss_fn, device): \n",
        "  \n",
        "  model.eval() # перевод состояния модели в \"не будем обучать\"\n",
        "\n",
        "  avg_loss = 0 # накопленное значение ошибки\n",
        "  num_batches = len(dataloader) # количество батчей в тестовом датасете\n",
        "  correct, total = 0, 0 # счетчик верных предсказаний\n",
        "  \n",
        "  for x, y in dataloader:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    pred = model(x)\n",
        "    avg_loss += loss_fn(pred, y)\n",
        "    y_test = torch.flatten(y)\n",
        "    y_pred = torch.flatten(pred.argmax(1))\n",
        "    total += y_test.size(0)\n",
        "    correct += (y_pred == y_test).sum()\n",
        "    \n",
        "    del x, y, pred\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  avg_loss /= num_batches # считаем итоговую ошибку на шаге оценки модели\n",
        "  accuracy = correct / total # считаем точность (верно предсказанные на все данные)\n",
        "  \n",
        "  print(f\"\\tAccuracy: {accuracy:>4f}, Avg Loss: {avg_loss:>8f}\")\n",
        "  return avg_loss, accuracy\n",
        "\n",
        "def common(model, loss_fn, optimizer, train_dataloader, epochs, test_dataloader, verbose, on_epoch_end, device):\n",
        "  \n",
        "  train_losses = [] # ошибки при обучении\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}\\n\" + \"_\" * 40)\n",
        "    train_loss = train_func(train_dataloader, model, loss_fn, optimizer, verbose=verbose, device=device) # сделали один шаг обучения\n",
        "    train_losses.append(train_loss.item()) # добавили значение ошибки по итогу шага обучения\n",
        "    if test_dataloader:\n",
        "      test_func(test_dataloader, model, loss_fn, device=device) # на каждом шаге обучения делать оценку модели\n",
        "    if on_epoch_end:\n",
        "      on_epoch_end()\n",
        "    print()\n",
        "    torch.cuda.empty_cache()\n",
        "  return train_losses"
      ],
      "metadata": {
        "id": "SVUvXJS9dTXD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def get_y_test_y_pred(model, test_dataloader, device):\n",
        "  \n",
        "  model.eval()\n",
        "  \n",
        "  y_test = []\n",
        "  y_pred = []\n",
        "  \n",
        "  for x, y in test_dataloader:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    pred = model(x).argmax(1)\n",
        "    y_test.append(y)\n",
        "    y_pred.append(pred)\n",
        "    \n",
        "    del x\n",
        "    torch.cuda.empty_cache()\n",
        "  \n",
        "  return torch.flatten(torch.vstack(y_test).detach().cpu()), torch.flatten(torch.vstack(y_pred).detach().cpu())"
      ],
      "metadata": {
        "id": "rr5N6fnYv7A5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "_ = common(\n",
        "    epochs=100,\n",
        "    model=names_gen_net,\n",
        "    loss_fn=loss_fn,\n",
        "    optimizer=optimizer,\n",
        "    train_dataloader=train_dataloader,\n",
        "    test_dataloader=test_dataloader,\n",
        "    verbose=50,\n",
        "    on_epoch_end=name_generate_at_end_of_epoch(names_gen_net, ndataset),\n",
        "    device=DEVICE\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx6Yi139wh_0",
        "outputId": "93a26aae-ad78-47c8-bb23-b8ff6b5a3d3c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "________________________________________\n",
            "loss: 3.517869  [    0 /  1590]\n",
            "\tAccuracy: 0.560069, Avg Loss: 1.799209\n",
            "Имена: нана (max), знлоягяпя<PAD>рр (prob), фцс<PAD>в<PAD> (softmax)\n",
            "\n",
            "Epoch 2\n",
            "________________________________________\n",
            "loss: 2.006588  [    0 /  1590]\n",
            "\tAccuracy: 0.646354, Avg Loss: 1.257461\n",
            "Имена: лена (max), тчтшшнюумыин (prob), мьнаина (softmax)\n",
            "\n",
            "Epoch 3\n",
            "________________________________________\n",
            "loss: 1.256164  [    0 /  1590]\n",
            "\tAccuracy: 0.666493, Avg Loss: 1.135421\n",
            "Имена: нина (max), пкрузхтяо<SOS>оо (prob), аидалия (softmax)\n",
            "\n",
            "Epoch 4\n",
            "________________________________________\n",
            "loss: 1.112189  [    0 /  1590]\n",
            "\tAccuracy: 0.672222, Avg Loss: 1.084103\n",
            "Имена: нана (max), эонгеь (prob), лооадя (softmax)\n",
            "\n",
            "Epoch 5\n",
            "________________________________________\n",
            "loss: 1.160003  [    0 /  1590]\n",
            "\tAccuracy: 0.684028, Avg Loss: 1.042555\n",
            "Имена: нана (max), мюьбтвпкшсой (prob), ниса (softmax)\n",
            "\n",
            "Epoch 6\n",
            "________________________________________\n",
            "loss: 1.015666  [    0 /  1590]\n",
            "\tAccuracy: 0.687674, Avg Loss: 1.013196\n",
            "Имена: наня (max), егхфнх<PAD>вяевт (prob), зктийтя (softmax)\n",
            "\n",
            "Epoch 7\n",
            "________________________________________\n",
            "loss: 0.994216  [    0 /  1590]\n",
            "\tAccuracy: 0.692535, Avg Loss: 0.990799\n",
            "Имена: нанина (max), ждтшпояайнюл (prob), алима (softmax)\n",
            "\n",
            "Epoch 8\n",
            "________________________________________\n",
            "loss: 1.073421  [    0 /  1590]\n",
            "\tAccuracy: 0.695139, Avg Loss: 0.968280\n",
            "Имена: наня (max), дфиушш<PAD>чтьсв (prob), пахитка (softmax)\n",
            "\n",
            "Epoch 9\n",
            "________________________________________\n",
            "loss: 1.112052  [    0 /  1590]\n",
            "\tAccuracy: 0.694965, Avg Loss: 0.955607\n",
            "Имена: наня (max), рскмжкиийхйп (prob), мааньчна (softmax)\n",
            "\n",
            "Epoch 10\n",
            "________________________________________\n",
            "loss: 0.988310  [    0 /  1590]\n",
            "\tAccuracy: 0.701563, Avg Loss: 0.942293\n",
            "Имена: наня (max), дииузабдффлт (prob), иськис (softmax)\n",
            "\n",
            "Epoch 11\n",
            "________________________________________\n",
            "loss: 1.007226  [    0 /  1590]\n",
            "\tAccuracy: 0.704687, Avg Loss: 0.930948\n",
            "Имена: наша (max),  (prob), нуса (softmax)\n",
            "\n",
            "Epoch 12\n",
            "________________________________________\n",
            "loss: 0.923586  [    0 /  1590]\n",
            "\tAccuracy: 0.709549, Avg Loss: 0.914078\n",
            "Имена: наня (max), лдхумичявнда (prob), нухима (softmax)\n",
            "\n",
            "Epoch 13\n",
            "________________________________________\n",
            "loss: 1.039597  [    0 /  1590]\n",
            "\tAccuracy: 0.708854, Avg Loss: 0.905472\n",
            "Имена: нася (max), имоворынх (prob), бгопя (softmax)\n",
            "\n",
            "Epoch 14\n",
            "________________________________________\n",
            "loss: 0.939050  [    0 /  1590]\n",
            "\tAccuracy: 0.711458, Avg Loss: 0.896107\n",
            "Имена: наня (max), мвзжиосйеосч (prob), ахней (softmax)\n",
            "\n",
            "Epoch 15\n",
            "________________________________________\n",
            "loss: 0.905773  [    0 /  1590]\n",
            "\tAccuracy: 0.711806, Avg Loss: 0.889317\n",
            "Имена: наня (max), всуряхпбнруз (prob), слихланы<PAD><PAD><PAD><PAD> (softmax)\n",
            "\n",
            "Epoch 16\n",
            "________________________________________\n",
            "loss: 0.854277  [    0 /  1590]\n",
            "\tAccuracy: 0.716146, Avg Loss: 0.885274\n",
            "Имена: наня (max), <UNK>ялохмвкржни (prob), димюся (softmax)\n",
            "\n",
            "Epoch 17\n",
            "________________________________________\n",
            "loss: 0.938733  [    0 /  1590]\n",
            "\tAccuracy: 0.719444, Avg Loss: 0.872873\n",
            "Имена: нася (max), вгис<SOS>явглюдп (prob), митоха (softmax)\n",
            "\n",
            "Epoch 18\n",
            "________________________________________\n",
            "loss: 0.951358  [    0 /  1590]\n",
            "\tAccuracy: 0.722222, Avg Loss: 0.867235\n",
            "Имена: ника (max), йошвхяхиярюс (prob), каня (softmax)\n",
            "\n",
            "Epoch 19\n",
            "________________________________________\n",
            "loss: 0.931809  [    0 /  1590]\n",
            "\tAccuracy: 0.722222, Avg Loss: 0.863498\n",
            "Имена: маня (max), еи (prob), наха (softmax)\n",
            "\n",
            "Epoch 20\n",
            "________________________________________\n",
            "loss: 0.906004  [    0 /  1590]\n",
            "\tAccuracy: 0.727604, Avg Loss: 0.853415\n",
            "Имена: маня (max), думвхе<PAD>дапит (prob), тара (softmax)\n",
            "\n",
            "Epoch 21\n",
            "________________________________________\n",
            "loss: 0.879945  [    0 /  1590]\n",
            "\tAccuracy: 0.723958, Avg Loss: 0.850061\n",
            "Имена: ника (max), пюафьпхьыуця (prob), михьыя (softmax)\n",
            "\n",
            "Epoch 22\n",
            "________________________________________\n",
            "loss: 0.847371  [    0 /  1590]\n",
            "\tAccuracy: 0.729861, Avg Loss: 0.841704\n",
            "Имена: маня (max), мань (prob), симела (softmax)\n",
            "\n",
            "Epoch 23\n",
            "________________________________________\n",
            "loss: 0.821686  [    0 /  1590]\n",
            "\tAccuracy: 0.731424, Avg Loss: 0.840086\n",
            "Имена: марина (max), мтьттлыыькжн (prob), сврдин (softmax)\n",
            "\n",
            "Epoch 24\n",
            "________________________________________\n",
            "loss: 0.863972  [    0 /  1590]\n",
            "\tAccuracy: 0.733854, Avg Loss: 0.832353\n",
            "Имена: натя (max), тгаеше (prob), меириж (softmax)\n",
            "\n",
            "Epoch 25\n",
            "________________________________________\n",
            "loss: 0.868514  [    0 /  1590]\n",
            "\tAccuracy: 0.735937, Avg Loss: 0.827436\n",
            "Имена: марина (max), сзэлар (prob), мизн (softmax)\n",
            "\n",
            "Epoch 26\n",
            "________________________________________\n",
            "loss: 0.897005  [    0 /  1590]\n",
            "\tAccuracy: 0.736458, Avg Loss: 0.823685\n",
            "Имена: милина (max), идлафр (prob), вася (softmax)\n",
            "\n",
            "Epoch 27\n",
            "________________________________________\n",
            "loss: 0.845576  [    0 /  1590]\n",
            "\tAccuracy: 0.734549, Avg Loss: 0.824612\n",
            "Имена: ника (max), апблмдя<SOS>пвун (prob), даша (softmax)\n",
            "\n",
            "Epoch 28\n",
            "________________________________________\n",
            "loss: 0.826690  [    0 /  1590]\n",
            "\tAccuracy: 0.737674, Avg Loss: 0.818839\n",
            "Имена: ника (max), лвст (prob), луся (softmax)\n",
            "\n",
            "Epoch 29\n",
            "________________________________________\n",
            "loss: 0.847067  [    0 /  1590]\n",
            "\tAccuracy: 0.740625, Avg Loss: 0.808483\n",
            "Имена: ника (max), юфюрюксийшюд (prob), ка (softmax)\n",
            "\n",
            "Epoch 30\n",
            "________________________________________\n",
            "loss: 0.814816  [    0 /  1590]\n",
            "\tAccuracy: 0.743924, Avg Loss: 0.810157\n",
            "Имена: ника (max), хлсжжюшйбодв (prob), вася (softmax)\n",
            "\n",
            "Epoch 31\n",
            "________________________________________\n",
            "loss: 0.845420  [    0 /  1590]\n",
            "\tAccuracy: 0.746528, Avg Loss: 0.804127\n",
            "Имена: лина (max), ртбгервяеоьб (prob), амгейин (softmax)\n",
            "\n",
            "Epoch 32\n",
            "________________________________________\n",
            "loss: 0.734368  [    0 /  1590]\n",
            "\tAccuracy: 0.747569, Avg Loss: 0.799350\n",
            "Имена: лина (max), тсжяиабтаюрн (prob), мена (softmax)\n",
            "\n",
            "Epoch 33\n",
            "________________________________________\n",
            "loss: 0.798338  [    0 /  1590]\n",
            "\tAccuracy: 0.744097, Avg Loss: 0.796724\n",
            "Имена: лина (max), зса (prob), синыч (softmax)\n",
            "\n",
            "Epoch 34\n",
            "________________________________________\n",
            "loss: 0.790022  [    0 /  1590]\n",
            "\tAccuracy: 0.750174, Avg Loss: 0.796373\n",
            "Имена: лина (max), паюсушьершик (prob), калексенилиа (softmax)\n",
            "\n",
            "Epoch 35\n",
            "________________________________________\n",
            "loss: 0.824508  [    0 /  1590]\n",
            "\tAccuracy: 0.749826, Avg Loss: 0.792873\n",
            "Имена: лина (max), зкыо (prob), хрисин (softmax)\n",
            "\n",
            "Epoch 36\n",
            "________________________________________\n",
            "loss: 0.871004  [    0 /  1590]\n",
            "\tAccuracy: 0.748438, Avg Loss: 0.788911\n",
            "Имена: лина (max), зьйрюбнбшэан (prob), деня (softmax)\n",
            "\n",
            "Epoch 37\n",
            "________________________________________\n",
            "loss: 0.801569  [    0 /  1590]\n",
            "\tAccuracy: 0.748264, Avg Loss: 0.785777\n",
            "Имена: лина (max), палккис<SOS>салс (prob), настаславка (softmax)\n",
            "\n",
            "Epoch 38\n",
            "________________________________________\n",
            "loss: 0.815003  [    0 /  1590]\n",
            "\tAccuracy: 0.750521, Avg Loss: 0.789720\n",
            "Имена: лина (max), мчяшжы<SOS>есдяу (prob), режа (softmax)\n",
            "\n",
            "Epoch 39\n",
            "________________________________________\n",
            "loss: 0.813063  [    0 /  1590]\n",
            "\tAccuracy: 0.749826, Avg Loss: 0.778979\n",
            "Имена: лина (max), мдбтраи (prob), стеморя (softmax)\n",
            "\n",
            "Epoch 40\n",
            "________________________________________\n",
            "loss: 0.798119  [    0 /  1590]\n",
            "\tAccuracy: 0.753299, Avg Loss: 0.781732\n",
            "Имена: лина (max), плтгпафя<PAD>шт<UNK> (prob), лявтона (softmax)\n",
            "\n",
            "Epoch 41\n",
            "________________________________________\n",
            "loss: 0.728791  [    0 /  1590]\n",
            "\tAccuracy: 0.751736, Avg Loss: 0.773952\n",
            "Имена: лина (max), поднцме<PAD>н<PAD>цс (prob), агелиныч (softmax)\n",
            "\n",
            "Epoch 42\n",
            "________________________________________\n",
            "loss: 0.733550  [    0 /  1590]\n",
            "\tAccuracy: 0.751042, Avg Loss: 0.774248\n",
            "Имена: лина (max), хлтаря<PAD>нтгпс (prob), кон (softmax)\n",
            "\n",
            "Epoch 43\n",
            "________________________________________\n",
            "loss: 0.749573  [    0 /  1590]\n",
            "\tAccuracy: 0.755382, Avg Loss: 0.774546\n",
            "Имена: вася (max), юлоаикцинмдь (prob), линоныч (softmax)\n",
            "\n",
            "Epoch 44\n",
            "________________________________________\n",
            "loss: 0.746383  [    0 /  1590]\n",
            "\tAccuracy: 0.753125, Avg Loss: 0.773978\n",
            "Имена: лина (max), хлшлцкаейжяс (prob), аксюша (softmax)\n",
            "\n",
            "Epoch 45\n",
            "________________________________________\n",
            "loss: 0.815021  [    0 /  1590]\n",
            "\tAccuracy: 0.753299, Avg Loss: 0.771348\n",
            "Имена: лина (max), моаэгиллюивт (prob), лавруша (softmax)\n",
            "\n",
            "Epoch 46\n",
            "________________________________________\n",
            "loss: 0.744705  [    0 /  1590]\n",
            "\tAccuracy: 0.756250, Avg Loss: 0.771131\n",
            "Имена: марина (max), фмжйжнахгпрт (prob), вениниса (softmax)\n",
            "\n",
            "Epoch 47\n",
            "________________________________________\n",
            "loss: 0.759146  [    0 /  1590]\n",
            "\tAccuracy: 0.759896, Avg Loss: 0.767734\n",
            "Имена: марина (max), чказаэуля (prob), гинуша (softmax)\n",
            "\n",
            "Epoch 48\n",
            "________________________________________\n",
            "loss: 0.721613  [    0 /  1590]\n",
            "\tAccuracy: 0.752083, Avg Loss: 0.769728\n",
            "Имена: митаня (max), вбл<SOS>пц<SOS>жьрзт (prob), пався (softmax)\n",
            "\n",
            "Epoch 49\n",
            "________________________________________\n",
            "loss: 0.711819  [    0 /  1590]\n",
            "\tAccuracy: 0.757639, Avg Loss: 0.765878\n",
            "Имена: марислав (max), дяухенбсяшйа (prob), тианыч (softmax)\n",
            "\n",
            "Epoch 50\n",
            "________________________________________\n",
            "loss: 0.737196  [    0 /  1590]\n",
            "\tAccuracy: 0.753472, Avg Loss: 0.764250\n",
            "Имена: митаня (max), феррикара (prob), анюша (softmax)\n",
            "\n",
            "Epoch 51\n",
            "________________________________________\n",
            "loss: 0.773158  [    0 /  1590]\n",
            "\tAccuracy: 0.758507, Avg Loss: 0.761360\n",
            "Имена: мариславка (max), чллыияб<SOS>гозп (prob), витюша (softmax)\n",
            "\n",
            "Epoch 52\n",
            "________________________________________\n",
            "loss: 0.717513  [    0 /  1590]\n",
            "\tAccuracy: 0.756597, Avg Loss: 0.761334\n",
            "Имена: митаня (max), юуееосуытлах (prob), антония (softmax)\n",
            "\n",
            "Epoch 53\n",
            "________________________________________\n",
            "loss: 0.752517  [    0 /  1590]\n",
            "\tAccuracy: 0.758333, Avg Loss: 0.760742\n",
            "Имена: мариан (max), лйазтьрядмыа (prob), антон (softmax)\n",
            "\n",
            "Epoch 54\n",
            "________________________________________\n",
            "loss: 0.744888  [    0 /  1590]\n",
            "\tAccuracy: 0.756944, Avg Loss: 0.760525\n",
            "Имена: мариславка (max), ма (prob), веронинка (softmax)\n",
            "\n",
            "Epoch 55\n",
            "________________________________________\n",
            "loss: 0.748392  [    0 /  1590]\n",
            "\tAccuracy: 0.756597, Avg Loss: 0.754726\n",
            "Имена: марислав (max), нэнжьюкткэхи (prob), нетяша (softmax)\n",
            "\n",
            "Epoch 56\n",
            "________________________________________\n",
            "loss: 0.703752  [    0 /  1590]\n",
            "\tAccuracy: 0.759028, Avg Loss: 0.755344\n",
            "Имена: марина (max), фвушеа<PAD>шн<UNK>ьы (prob), вена (softmax)\n",
            "\n",
            "Epoch 57\n",
            "________________________________________\n",
            "loss: 0.725827  [    0 /  1590]\n",
            "\tAccuracy: 0.760243, Avg Loss: 0.759239\n",
            "Имена: марися (max), гилц<PAD>ираййиу (prob), ига (softmax)\n",
            "\n",
            "Epoch 58\n",
            "________________________________________\n",
            "loss: 0.731203  [    0 /  1590]\n",
            "\tAccuracy: 0.762153, Avg Loss: 0.751818\n",
            "Имена: марислав (max), упбрм (prob), льгя (softmax)\n",
            "\n",
            "Epoch 59\n",
            "________________________________________\n",
            "loss: 0.687458  [    0 /  1590]\n",
            "\tAccuracy: 0.759375, Avg Loss: 0.754170\n",
            "Имена: линуша (max), фуоо<PAD>юю (prob), степка (softmax)\n",
            "\n",
            "Epoch 60\n",
            "________________________________________\n",
            "loss: 0.689358  [    0 /  1590]\n",
            "\tAccuracy: 0.751910, Avg Loss: 0.754119\n",
            "Имена: лина (max), зфхчтх (prob), тарим (softmax)\n",
            "\n",
            "Epoch 61\n",
            "________________________________________\n",
            "loss: 0.765775  [    0 /  1590]\n",
            "\tAccuracy: 0.752951, Avg Loss: 0.756324\n",
            "Имена: лина (max), нслвпсг (prob), маша (softmax)\n",
            "\n",
            "Epoch 62\n",
            "________________________________________\n",
            "loss: 0.742999  [    0 /  1590]\n",
            "\tAccuracy: 0.754861, Avg Loss: 0.754533\n",
            "Имена: лина (max), схввюнююфша<PAD> (prob), иваса (softmax)\n",
            "\n",
            "Epoch 63\n",
            "________________________________________\n",
            "loss: 0.732064  [    0 /  1590]\n",
            "\tAccuracy: 0.753819, Avg Loss: 0.751336\n",
            "Имена: лина (max), пявтяпвьцбьл (prob), сергия (softmax)\n",
            "\n",
            "Epoch 64\n",
            "________________________________________\n",
            "loss: 0.681612  [    0 /  1590]\n",
            "\tAccuracy: 0.758681, Avg Loss: 0.751717\n",
            "Имена: марися (max), ихплйхчоошол (prob), ледя (softmax)\n",
            "\n",
            "Epoch 65\n",
            "________________________________________\n",
            "loss: 0.723843  [    0 /  1590]\n",
            "\tAccuracy: 0.762674, Avg Loss: 0.748933\n",
            "Имена: мариславка (max), пюлбрыдгбв<PAD>н (prob), павлик (softmax)\n",
            "\n",
            "Epoch 66\n",
            "________________________________________\n",
            "loss: 0.664325  [    0 /  1590]\n",
            "\tAccuracy: 0.757812, Avg Loss: 0.754359\n",
            "Имена: митрий (max), эв<PAD>ыйбфвтн<UNK>е (prob), эм (softmax)\n",
            "\n",
            "Epoch 67\n",
            "________________________________________\n",
            "loss: 0.658571  [    0 /  1590]\n",
            "\tAccuracy: 0.760764, Avg Loss: 0.749018\n",
            "Имена: митрий (max), торюхнд<PAD>лкад (prob), елей (softmax)\n",
            "\n",
            "Epoch 68\n",
            "________________________________________\n",
            "loss: 0.645666  [    0 /  1590]\n",
            "\tAccuracy: 0.756771, Avg Loss: 0.754506\n",
            "Имена: митрий (max), йошвпвстьякд (prob), юра (softmax)\n",
            "\n",
            "Epoch 69\n",
            "________________________________________\n",
            "loss: 0.695069  [    0 /  1590]\n",
            "\tAccuracy: 0.760938, Avg Loss: 0.749772\n",
            "Имена: мариан (max), жж<PAD>е<SOS>эубитер (prob), геха (softmax)\n",
            "\n",
            "Epoch 70\n",
            "________________________________________\n",
            "loss: 0.650087  [    0 /  1590]\n",
            "\tAccuracy: 0.761458, Avg Loss: 0.752835\n",
            "Имена: марисьяна (max), з (prob), авдокийтыч (softmax)\n",
            "\n",
            "Epoch 71\n",
            "________________________________________\n",
            "loss: 0.692948  [    0 /  1590]\n",
            "\tAccuracy: 0.758333, Avg Loss: 0.750916\n",
            "Имена: марися (max), рмишор (prob), ила (softmax)\n",
            "\n",
            "Epoch 72\n",
            "________________________________________\n",
            "loss: 0.665692  [    0 /  1590]\n",
            "\tAccuracy: 0.762847, Avg Loss: 0.753009\n",
            "Имена: марисьяна (max), бэкмуиаринаа (prob), елинка (softmax)\n",
            "\n",
            "Epoch 73\n",
            "________________________________________\n",
            "loss: 0.681821  [    0 /  1590]\n",
            "\tAccuracy: 0.762847, Avg Loss: 0.747595\n",
            "Имена: мариан (max), молелкнррбтв (prob), ирия (softmax)\n",
            "\n",
            "Epoch 74\n",
            "________________________________________\n",
            "loss: 0.701724  [    0 /  1590]\n",
            "\tAccuracy: 0.757118, Avg Loss: 0.752097\n",
            "Имена: митрий (max), ривзаваныеич (prob), мия (softmax)\n",
            "\n",
            "Epoch 75\n",
            "________________________________________\n",
            "loss: 0.724224  [    0 /  1590]\n",
            "\tAccuracy: 0.760417, Avg Loss: 0.749800\n",
            "Имена: миланя (max), уяакоэьшедуж (prob), албина (softmax)\n",
            "\n",
            "Epoch 76\n",
            "________________________________________\n",
            "loss: 0.674501  [    0 /  1590]\n",
            "\tAccuracy: 0.760417, Avg Loss: 0.747306\n",
            "Имена: марисьяна (max), гиоздошж (prob), гейльыа (softmax)\n",
            "\n",
            "Epoch 77\n",
            "________________________________________\n",
            "loss: 0.671475  [    0 /  1590]\n",
            "\tAccuracy: 0.759375, Avg Loss: 0.746057\n",
            "Имена: митрий (max), мйрю (prob), коля (softmax)\n",
            "\n",
            "Epoch 78\n",
            "________________________________________\n",
            "loss: 0.684070  [    0 /  1590]\n",
            "\tAccuracy: 0.766319, Avg Loss: 0.747261\n",
            "Имена: мариам (max), ьт (prob), лареха (softmax)\n",
            "\n",
            "Epoch 79\n",
            "________________________________________\n",
            "loss: 0.672877  [    0 /  1590]\n",
            "\tAccuracy: 0.763368, Avg Loss: 0.748402\n",
            "Имена: марися (max), дтйбнэул (prob), эдка (softmax)\n",
            "\n",
            "Epoch 80\n",
            "________________________________________\n",
            "loss: 0.688434  [    0 /  1590]\n",
            "\tAccuracy: 0.761979, Avg Loss: 0.746936\n",
            "Имена: марися (max), еквюхмж (prob), сена (softmax)\n",
            "\n",
            "Epoch 81\n",
            "________________________________________\n",
            "loss: 0.692411  [    0 /  1590]\n",
            "\tAccuracy: 0.763021, Avg Loss: 0.746551\n",
            "Имена: марися (max), вк<UNK> (prob), тая (softmax)\n",
            "\n",
            "Epoch 82\n",
            "________________________________________\n",
            "loss: 0.632858  [    0 /  1590]\n",
            "\tAccuracy: 0.760590, Avg Loss: 0.746333\n",
            "Имена: митрий (max), ффгопцлктр<PAD> (prob), косся (softmax)\n",
            "\n",
            "Epoch 83\n",
            "________________________________________\n",
            "loss: 0.652037  [    0 /  1590]\n",
            "\tAccuracy: 0.764931, Avg Loss: 0.747395\n",
            "Имена: марися (max), форлусяюгкен (prob), линка (softmax)\n",
            "\n",
            "Epoch 84\n",
            "________________________________________\n",
            "loss: 0.708013  [    0 /  1590]\n",
            "\tAccuracy: 0.758507, Avg Loss: 0.745353\n",
            "Имена: митрий (max), жвусцчбцл<PAD>ьт (prob), мирринка (softmax)\n",
            "\n",
            "Epoch 85\n",
            "________________________________________\n",
            "loss: 0.681446  [    0 /  1590]\n",
            "\tAccuracy: 0.763542, Avg Loss: 0.742413\n",
            "Имена: мариана (max), кереаолароех (prob), лоня (softmax)\n",
            "\n",
            "Epoch 86\n",
            "________________________________________\n",
            "loss: 0.640911  [    0 /  1590]\n",
            "\tAccuracy: 0.764757, Avg Loss: 0.748551\n",
            "Имена: василиска (max), л (prob), дикуся (softmax)\n",
            "\n",
            "Epoch 87\n",
            "________________________________________\n",
            "loss: 0.632696  [    0 /  1590]\n",
            "\tAccuracy: 0.765625, Avg Loss: 0.746131\n",
            "Имена: марианка (max), ивва<PAD><SOS>бк<SOS>свн (prob), виоленка (softmax)\n",
            "\n",
            "Epoch 88\n",
            "________________________________________\n",
            "loss: 0.657386  [    0 /  1590]\n",
            "\tAccuracy: 0.762847, Avg Loss: 0.751394\n",
            "Имена: марина (max), тгбон<PAD>кр<PAD>шее (prob), юлич (softmax)\n",
            "\n",
            "Epoch 89\n",
            "________________________________________\n",
            "loss: 0.712857  [    0 /  1590]\n",
            "\tAccuracy: 0.762153, Avg Loss: 0.753274\n",
            "Имена: марьянка (max), пр<PAD>олшяхаауа (prob), степаша (softmax)\n",
            "\n",
            "Epoch 90\n",
            "________________________________________\n",
            "loss: 0.693075  [    0 /  1590]\n",
            "\tAccuracy: 0.761806, Avg Loss: 0.749821\n",
            "Имена: митрич (max), уоммбйа<UNK> (prob), иреинка (softmax)\n",
            "\n",
            "Epoch 91\n",
            "________________________________________\n",
            "loss: 0.673158  [    0 /  1590]\n",
            "\tAccuracy: 0.767882, Avg Loss: 0.747610\n",
            "Имена: мариан (max), сошоижгирэтч (prob), лилиан (softmax)\n",
            "\n",
            "Epoch 92\n",
            "________________________________________\n",
            "loss: 0.686011  [    0 /  1590]\n",
            "\tAccuracy: 0.764063, Avg Loss: 0.749405\n",
            "Имена: марина (max), саяружьорл (prob), хринуста (softmax)\n",
            "\n",
            "Epoch 93\n",
            "________________________________________\n",
            "loss: 0.670640  [    0 /  1590]\n",
            "\tAccuracy: 0.762326, Avg Loss: 0.749725\n",
            "Имена: марина (max), мчйоекхевн<UNK>л (prob), нина (softmax)\n",
            "\n",
            "Epoch 94\n",
            "________________________________________\n",
            "loss: 0.654795  [    0 /  1590]\n",
            "\tAccuracy: 0.767187, Avg Loss: 0.749138\n",
            "Имена: мариан (max), лсегжкуяйиуш (prob), сариамч (softmax)\n",
            "\n",
            "Epoch 95\n",
            "________________________________________\n",
            "loss: 0.635950  [    0 /  1590]\n",
            "\tAccuracy: 0.766493, Avg Loss: 0.750587\n",
            "Имена: антонинка (max), тфкобрьфпеюы (prob), ледя (softmax)\n",
            "\n",
            "Epoch 96\n",
            "________________________________________\n",
            "loss: 0.687843  [    0 /  1590]\n",
            "\tAccuracy: 0.766493, Avg Loss: 0.747897\n",
            "Имена: марианка (max), сфждтьпидыьс (prob), славемила (softmax)\n",
            "\n",
            "Epoch 97\n",
            "________________________________________\n",
            "loss: 0.627760  [    0 /  1590]\n",
            "\tAccuracy: 0.767882, Avg Loss: 0.744528\n",
            "Имена: мариана (max), уузсвсашаяшг (prob), сарлина (softmax)\n",
            "\n",
            "Epoch 98\n",
            "________________________________________\n",
            "loss: 0.674223  [    0 /  1590]\n",
            "\tAccuracy: 0.768229, Avg Loss: 0.754775\n",
            "Имена: мариан (max), нюяялкмд<PAD>кьк (prob), авка (softmax)\n",
            "\n",
            "Epoch 99\n",
            "________________________________________\n",
            "loss: 0.646149  [    0 /  1590]\n",
            "\tAccuracy: 0.764063, Avg Loss: 0.753469\n",
            "Имена: антонинка (max), азвфлочшодап (prob), степаний (softmax)\n",
            "\n",
            "Epoch 100\n",
            "________________________________________\n",
            "loss: 0.665261  [    0 /  1590]\n",
            "\tAccuracy: 0.763715, Avg Loss: 0.753108\n",
            "Имена: мариан (max),  (prob), миронка (softmax)\n",
            "\n",
            "CPU times: user 32.3 s, sys: 701 ms, total: 33 s\n",
            "Wall time: 33.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test, y_pred = get_y_test_y_pred(names_gen_net, test_dataloader, DEVICE)\n",
        "print(metrics.classification_report(y_true=y_test, y_pred=y_pred, target_names=[ndataset.vocab.symbols[i] for i in y_test.unique().sort()[0]], zero_division=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X_9OW03wuNO",
        "outputId": "def3718f-5539-4f61-b34d-1753dab58fd4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <PAD>       1.00      1.00      1.00      3009\n",
            "       <EOS>       0.81      0.93      0.87       384\n",
            "           д       0.62      0.38      0.48        52\n",
            "           с       0.51      0.37      0.43       103\n",
            "           т       0.59      0.58      0.58       113\n",
            "           р       0.54      0.53      0.53       108\n",
            "           ю       0.24      0.11      0.15        66\n",
            "           й       0.50      0.21      0.30        19\n",
            "           я       0.56      0.48      0.52       134\n",
            "           л       0.50      0.49      0.50       156\n",
            "           и       0.40      0.41      0.40       176\n",
            "           ш       0.26      0.23      0.24        52\n",
            "           б       0.40      0.29      0.33        14\n",
            "           у       0.30      0.13      0.18        60\n",
            "           ц       1.00      0.00      0.00         1\n",
            "           г       1.00      0.07      0.12        30\n",
            "           ч       0.74      0.82      0.78        28\n",
            "           п       0.67      0.24      0.35        34\n",
            "           а       0.56      0.63      0.59       442\n",
            "           ж       0.00      0.00      0.00         4\n",
            "           ф       0.00      0.00      0.00        11\n",
            "           о       0.49      0.61      0.54        71\n",
            "           ь       0.38      0.19      0.25        27\n",
            "           з       0.50      0.17      0.25         6\n",
            "           м       0.22      0.54      0.32        81\n",
            "           х       0.17      0.02      0.04        44\n",
            "           ы       1.00      0.00      0.00        19\n",
            "           н       0.49      0.43      0.46       206\n",
            "           в       0.17      0.38      0.24        76\n",
            "           к       0.26      0.18      0.22        92\n",
            "           е       0.46      0.49      0.47       136\n",
            "           э       1.00      0.00      0.00         6\n",
            "\n",
            "    accuracy                           0.76      5760\n",
            "   macro avg       0.51      0.34      0.35      5760\n",
            "weighted avg       0.77      0.76      0.76      5760\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "По итогу, модель научилась предсказывать `<PAD>` и `<EOS>`. Класс!"
      ],
      "metadata": {
        "id": "XXT0OtR8DdSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(name_generate(names_gen_net, ndataset, device=DEVICE))\n",
        "print(name_generate(names_gen_net, ndataset, prob=softmax_prob, device=DEVICE))\n",
        "print(name_generate(names_gen_net, ndataset, prompt=\"ив\", device=DEVICE))\n",
        "print(name_generate(names_gen_net, ndataset, prompt=\"ив\", prob=softmax_prob, device=DEVICE))\n",
        "print(name_generate(names_gen_net, ndataset, prompt=\"анг\", device=DEVICE))\n",
        "print(name_generate(names_gen_net, ndataset, prompt=\"анг\", prob=softmax_prob, device=DEVICE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzx5gawXDcqR",
        "outputId": "2431e2ef-da18-4829-ce46-11d70aaae21e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "мариан\n",
            "мела\n",
            "иван\n",
            "ивория\n",
            "ангелина\n",
            "ангелина\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Уау! Круто же!"
      ],
      "metadata": {
        "id": "NYrfpqPEEGo9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJf5iaA2fOTM"
      },
      "source": [
        "## 2. Генерирование текста при помощи RNN\n",
        "\n",
        "2.1 Скачайте из интернета какое-нибудь художественное произведение\n",
        "  * Выбирайте достаточно крупное произведение, чтобы модель лучше обучалась;\n",
        "\n",
        "2.2 На основе выбранного произведения создайте датасет. \n",
        "\n",
        "Отличия от задачи 1:\n",
        "  * Токены <SOS>, `<EOS>` и `<UNK>` можно не добавлять;\n",
        "  * При создании датасета текст необходимо предварительно разбить на части. Выберите желаемую длину последовательности `seq_len` и разбейте текст на построки длины `seq_len` (можно без перекрытия, можно с небольшим перекрытием).\n",
        "\n",
        "2.3 Создайте и обучите модель для генерации текста\n",
        "  * Задача ставится точно так же как в 1.2;\n",
        "  * При необходимости можете применить:\n",
        "    * двухуровневые рекуррентные слои (`num_layers`=2)\n",
        "    * [обрезку градиентов](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
        "\n",
        "2.4 Напишите функцию, которая генерирует фрагмент текста при помощи обученной модели\n",
        "  * Процесс генерации начинается с небольшого фрагмента текста `prime`, выбранного вами (1-2 слова) \n",
        "  * Сначала вы пропускаете через модель токены из `prime` и генерируете на их основе скрытое состояние рекуррентного слоя `h_t`;\n",
        "  * После этого вы генерируете строку нужной длины аналогично 1.3\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextVocab:\n",
        "  PAD = \"<PAD>\"\n",
        "  PAD_IDX = 0\n",
        "  UNK = \"<UNK>\"\n",
        "  UNK_IDX = 1\n",
        "  \n",
        "  def __init__(self, seqs: t.List[str]):\n",
        "    unique_list = set()\n",
        "    max_seq_len = 0\n",
        "    for seq in map(str.lower, seqs):\n",
        "      unique_list.update(seq)\n",
        "      max_seq_len = max(len(seq), max_seq_len)\n",
        "    \n",
        "    self.symbols = [self.PAD, self.UNK, *unique_list]\n",
        "    self.max_seq_len = max_seq_len\n",
        "    symb_ind = {s: i for i, s in enumerate(self.symbols)}\n",
        "    self.symb_ind = defaultdict(lambda: self.UNK_IDX, symb_ind)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.symbols)\n",
        "  \n",
        "  def encode(self, seq: str) -> torch.Tensor:\n",
        "    indices = [self.symb_ind[s] for s in seq]\n",
        "    indices += [self.PAD_IDX] * (self.max_seq_len - len(indices))\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "  \n",
        "  def decode(self, indices: torch.Tensor) -> str:\n",
        "    pad_indices = torch.nonzero(indices == self.symb_ind[self.PAD], as_tuple=True)[0]\n",
        "    if len(pad_indices):\n",
        "      indices = indices[:pad_indices[0]]\n",
        "    return \"\".join(self.symbols[i] for i in indices)"
      ],
      "metadata": {
        "id": "HlNt65j0EUHq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset:\n",
        "  sequences: t.List[str]\n",
        "  vocab: TextVocab\n",
        "  data: torch.Tensor\n",
        "  targets: torch.Tensor\n",
        "  \n",
        "  def __init__(self, *paths, window, overlap):\n",
        "    self.sequences = self.get_sequences(*paths, window=window, overlap=overlap)\n",
        "    self.vocab = TextVocab(self.sequences)\n",
        "    self.vocab.max_seq_len -= 1\n",
        "    self.data = torch.vstack([self.encode(sequence[:-1]) for sequence in self.sequences])\n",
        "    self.targets = torch.vstack([self.encode(sequence[1:]) for sequence in self.sequences])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.data.size(0)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx], self.targets[idx]\n",
        "  \n",
        "  @staticmethod\n",
        "  def get_sequences(*paths, window, overlap):\n",
        "    text = \"\"\n",
        "    for path in paths:\n",
        "      with open(path) as f:\n",
        "        text += \" \" + \" \".join(map(lambda s: s.strip().lower(), f))\n",
        "    text = re.sub(r\"[^а-яё]\", repl=\" \", string=text)\n",
        "    text = text.replace(\"ё\", \"е\")\n",
        "    text = \" \".join(text.split())  # длинные пробелы\n",
        "    \n",
        "    sequences = []\n",
        "    \n",
        "    # последовательности длины window с перекрытием с обоих сторон\n",
        "    for i in range(0, len(text), window):\n",
        "      sequences.append(text[i:i + window + overlap])\n",
        "    \n",
        "    return sequences[:-1] # убираем неполную последовательность\n",
        "  \n",
        "  def encode(self, sequences: str):\n",
        "      return self.vocab.encode(sequences)\n",
        "  \n",
        "  def decode(self, indices: torch.Tensor):\n",
        "      return self.vocab.decode(indices)"
      ],
      "metadata": {
        "id": "rnIWojEnF_KQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path = filter(lambda x: x.suffix == \".txt\", (\"/content/drive/MyDrive/ML_FU/Lab_8/data/texts\").iterdir())\n",
        "# path = filter(lambda x: x.suffix == \".txt\", (\"/content/drive/MyDrive/ML_FU/Lab_8/data/texts\"))\n",
        "# path = Path(\"/content/drive/MyDrive/ML_FU/Lab_8/data/idiot.txt\").iterdir()\n",
        "path = [\"/content/drive/MyDrive/ML_FU/Lab_8/data/texts/idiot.txt\"]\n",
        "tdataset = TextDataset(*path, window=64, overlap=4)\n",
        "# tdataset = TextDataset([\"/content/drive/MyDrive/ML_FU/Lab_8/data/texts/idiot.txt\"], window=64, overlap=4)\n",
        "print(f\"n: {len(tdataset)}\")\n",
        "(tdataset.sequences[0], *tdataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZJ8hDbwIkrE",
        "outputId": "b72be15a-2f3b-4962-ad6d-21dc87ba8aa0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n: 19334\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('идиот роман в котором творческие принципы достоевского воплощаются в',\n",
              " tensor([10,  2, 10, 23,  4, 16,  3, 23, 28, 20, 30, 16, 31, 16, 32, 23,  4, 23,\n",
              "          3, 23, 28, 16,  4, 31, 23,  3, 18, 33,  5, 32, 10, 33, 16, 19,  3, 10,\n",
              "         30, 15, 10, 19, 27, 16,  2, 23,  5,  4, 23, 33, 31,  5, 32, 23, 17, 23,\n",
              "         16, 31, 23, 19,  9, 23, 25, 20,  6,  4,  5,  8, 16]),\n",
              " tensor([ 2, 10, 23,  4, 16,  3, 23, 28, 20, 30, 16, 31, 16, 32, 23,  4, 23,  3,\n",
              "         23, 28, 16,  4, 31, 23,  3, 18, 33,  5, 32, 10, 33, 16, 19,  3, 10, 30,\n",
              "         15, 10, 19, 27, 16,  2, 23,  5,  4, 23, 33, 31,  5, 32, 23, 17, 23, 16,\n",
              "         31, 23, 19,  9, 23, 25, 20,  6,  4,  5,  8, 16, 31]))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tdataset.sequences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JxO-39BLjvl",
        "outputId": "57ec3ad1-facd-46e5-ab8e-872e0a9b0db3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['идиот роман в котором творческие принципы достоевского воплощаются в',\n",
              " 'ся в полной мере а удивительное владение сюжетом достигает подлинног',\n",
              " 'нного расцвета яркая и почти болезненно талантливая история несчастн',\n",
              " 'астного князя мышкина неистового парфена рогожина и отчаявшейся наст',\n",
              " 'настасьи филипповны много раз экранизированная и поставленная на сце']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "train_tdataset, test_tdataset = split_train_test(tdataset, train_part=0.9)\n",
        "print(len(train_tdataset), len(test_tdataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlBez9_3LnFv",
        "outputId": "68be6005-55b7-430e-b0e2-0e016ddbe7f4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17401 1933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextRNNGenerator(nn.Module):\n",
        "  \n",
        "  _STATE = t.Union[t.Optional[torch.Tensor], t.Optional[t.Tuple[torch.Tensor, torch.Tensor]]]\n",
        "  rnn_state: _STATE\n",
        "  def __init__(self, num_embeddings, embedding_dim, rnn_hidden_size, rnn_cls):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=0)\n",
        "    self.rnn = rnn_cls(\n",
        "        input_size=embedding_dim,\n",
        "        hidden_size=rnn_hidden_size,\n",
        "        num_layers=2,\n",
        "        dropout=0.25,\n",
        "        batch_first=True,\n",
        "    )\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(rnn_hidden_size, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(256, num_embeddings),\n",
        "    )\n",
        "    \n",
        "    self.reset_rnn_state()\n",
        "  \n",
        "  def reset_rnn_state(self):\n",
        "    self.rnn_state = None\n",
        "  \n",
        "  def keep_rnn_state(self, state: _STATE):\n",
        "    if isinstance(self.rnn, nn.LSTM):\n",
        "      self.rnn_state = (state[0].detach(), state[1].detach())\n",
        "    else:\n",
        "      self.rnn_state = state.detach()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    x, rnn_state = self.rnn(x, self.rnn_state)\n",
        "    self.keep_rnn_state(rnn_state)\n",
        "    x = self.fc(x)\n",
        "    return x.permute(0, 2, 1)\n",
        "  \n",
        "  def train(self, mode: bool = True):\n",
        "    self.reset_rnn_state()\n",
        "    return super().train(mode)"
      ],
      "metadata": {
        "id": "n1ay0U_aMAOg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "text_gen_net = TextRNNGenerator(\n",
        "  num_embeddings=len(tdataset.vocab),\n",
        "  embedding_dim=12,\n",
        "  rnn_hidden_size=64,\n",
        "  rnn_cls=nn.LSTM,\n",
        ").to(DEVICE)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(text_gen_net.parameters(), lr=0.001)\n",
        "\n",
        "train_dataloader = DataLoader(train_tdataset, batch_size=128, shuffle=True, drop_last=True)\n",
        "test_dataloader = DataLoader(test_tdataset, batch_size=1024, drop_last=True)"
      ],
      "metadata": {
        "id": "xB_nabgxNZ6g"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "_ = common(\n",
        "  epochs=10,\n",
        "  model=text_gen_net,\n",
        "  loss_fn=loss_fn,\n",
        "  optimizer=optimizer,\n",
        "  train_dataloader=train_dataloader,\n",
        "  test_dataloader=test_dataloader,\n",
        "  verbose=500,\n",
        "  on_epoch_end= None,\n",
        "  device=DEVICE,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9kcIZBfNhHi",
        "outputId": "5c4e39fe-33c3-4878-b2d3-802323d43af0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "________________________________________\n",
            "loss: 3.561486  [    0 / 17401]\n",
            "\tAccuracy: 0.198242, Avg Loss: 2.793373\n",
            "\n",
            "Epoch 2\n",
            "________________________________________\n",
            "loss: 2.822891  [    0 / 17401]\n",
            "\tAccuracy: 0.283538, Avg Loss: 2.423970\n",
            "\n",
            "Epoch 3\n",
            "________________________________________\n",
            "loss: 2.464479  [    0 / 17401]\n",
            "\tAccuracy: 0.338124, Avg Loss: 2.233517\n",
            "\n",
            "Epoch 4\n",
            "________________________________________\n",
            "loss: 2.316159  [    0 / 17401]\n",
            "\tAccuracy: 0.369447, Avg Loss: 2.100693\n",
            "\n",
            "Epoch 5\n",
            "________________________________________\n",
            "loss: 2.197401  [    0 / 17401]\n",
            "\tAccuracy: 0.393759, Avg Loss: 2.009078\n",
            "\n",
            "Epoch 6\n",
            "________________________________________\n",
            "loss: 2.120177  [    0 / 17401]\n",
            "\tAccuracy: 0.411745, Avg Loss: 1.941374\n",
            "\n",
            "Epoch 7\n",
            "________________________________________\n",
            "loss: 2.040655  [    0 / 17401]\n",
            "\tAccuracy: 0.424222, Avg Loss: 1.889541\n",
            "\n",
            "Epoch 8\n",
            "________________________________________\n",
            "loss: 1.966584  [    0 / 17401]\n",
            "\tAccuracy: 0.435372, Avg Loss: 1.846305\n",
            "\n",
            "Epoch 9\n",
            "________________________________________\n",
            "loss: 1.974890  [    0 / 17401]\n",
            "\tAccuracy: 0.443709, Avg Loss: 1.811206\n",
            "\n",
            "Epoch 10\n",
            "________________________________________\n",
            "loss: 1.954624  [    0 / 17401]\n",
            "\tAccuracy: 0.450166, Avg Loss: 1.779867\n",
            "\n",
            "CPU times: user 3min 40s, sys: 8.01 s, total: 3min 48s\n",
            "Wall time: 3min 49s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видим, как точность растет"
      ],
      "metadata": {
        "id": "dZGBnyb6wzyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test, y_pred = get_y_test_y_pred(text_gen_net, test_dataloader, DEVICE)\n",
        "\n",
        "print(metrics.classification_report( y_true=y_test, y_pred=y_pred, target_names=[tdataset.vocab.symbols[i] for i in y_test.unique().sort()[0]], zero_division=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAY2wvOKNxCi",
        "outputId": "6078674a-d83b-430b-ba75-301a7edc4694"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           д       0.33      0.28      0.30      1687\n",
            "           р       0.58      0.25      0.35      2217\n",
            "           т       0.52      0.44      0.48      3674\n",
            "           с       0.26      0.15      0.19      2981\n",
            "           ю       1.00      0.00      0.00       346\n",
            "           й       0.35      0.24      0.29       569\n",
            "           я       0.68      0.44      0.53      1336\n",
            "           л       0.45      0.44      0.45      2587\n",
            "           и       0.34      0.21      0.26      3718\n",
            "           ш       0.62      0.16      0.26       488\n",
            "           б       0.48      0.11      0.17       947\n",
            "           у       0.69      0.11      0.18      1426\n",
            "           ъ       1.00      0.00      0.00        16\n",
            "           ц       0.64      0.13      0.21       164\n",
            "                   0.59      0.87      0.71     11571\n",
            "           г       0.36      0.23      0.28      1043\n",
            "           ч       0.71      0.07      0.13      1069\n",
            "           п       0.22      0.16      0.19      1548\n",
            "           а       0.44      0.45      0.44      4402\n",
            "           ж       0.39      0.37      0.38       671\n",
            "           ф       1.00      0.20      0.33       115\n",
            "           о       0.48      0.62      0.54      6482\n",
            "           ь       0.63      0.78      0.70      1290\n",
            "           щ       1.00      0.00      0.00       157\n",
            "           з       0.64      0.35      0.46       951\n",
            "           ы       0.51      0.28      0.36      1001\n",
            "           м       0.50      0.17      0.26      1805\n",
            "           х       1.00      0.00      0.00       414\n",
            "           н       0.20      0.54      0.29      3885\n",
            "           в       0.30      0.31      0.30      2691\n",
            "           к       0.68      0.25      0.36      1904\n",
            "           е       0.49      0.49      0.49      5226\n",
            "           э       1.00      0.00      0.00       227\n",
            "\n",
            "    accuracy                           0.45     68608\n",
            "   macro avg       0.58      0.28      0.30     68608\n",
            "weighted avg       0.48      0.45      0.43     68608\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Очень даже неплохая точность модели"
      ],
      "metadata": {
        "id": "6E8D1Y4Kw3Fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# где prompt - начало, size - длина текста\n",
        "def text_generation(model, dataset, prompt, size, prob, device):\n",
        "    \n",
        "  vocab = dataset.vocab\n",
        "  text_vec = [vocab.symb_ind[s] for s in prompt]\n",
        "  model.eval()\n",
        "  \n",
        "  for i in range(len(text_vec) - 1):\n",
        "    x = torch.tensor([[text_vec[i]]], device=device)\n",
        "      \n",
        "    model(x)\n",
        "  for i in range(size - len(text_vec)):\n",
        "    x = torch.tensor([[text_vec[-1]]], device=device)\n",
        "    pred = model(x).squeeze()\n",
        "    if prob:\n",
        "      next_s_idx = torch.multinomial(prob(pred), 1)\n",
        "    else:\n",
        "      next_s_idx = pred.argmax()\n",
        "    text_vec.append(next_s_idx.item())\n",
        "  return \"\".join(vocab.symbols[i] for i in text_vec)"
      ],
      "metadata": {
        "id": "HKOuMIHcN_6v"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for el in [\n",
        "  \"жили были\",\n",
        "  \"друг мой ты не стоишь этого\",\n",
        "  \"счастье было в том\"\n",
        "]:\n",
        "  print(el + \":\")\n",
        "  print(text_generation(text_gen_net, tdataset, el + \" \", 250, prob=softmax_prob, device=DEVICE), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZu3WdNdO2KD",
        "outputId": "5fce2f9d-55b2-4715-a659-8b1d62770424"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "жили были:\n",
            "жили были и большего вграгодий глеча сволупе столает к чтобы и мне стройно созначал князный жины и условственным гозлажать бой палыкоры то в сили про я вы митутю убудения князь вот то все то не зла когда и в позгиваю стал на ом захтать в кампитий пер \n",
            "\n",
            "друг мой ты не стоишь этого:\n",
            "друг мой ты не стоишь этого вдрур ее не оно фаллкужане в езездеходите им выладом изсколько свядий кам обедывила с по вь сипнем какие вам это вы он как к ихо вехорочно жестин было все не двамшенный князя одновно удер что подно княязи князь мило по пох \n",
            "\n",
            "счастье было в том:\n",
            "счастье было в том так и полюроку что незжолко тоже искрких был задом к том это был слого басякалче все какой общивом два мажен повля манет вас малере с видуть не рас другному сказать тих и могу дого ну счасться страсные хеет веловечившеская с посмра \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель не так плоха. Он научился генерировать новые \"слова\" и если сильно пофанатзировать, то это действительно похоже на слова, союзы и предлоги. Есть как несуществующие слова в тексте, так и настоящие \"был\", \"чтобы\"."
      ],
      "metadata": {
        "id": "RaH2LaZIGXtb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HKPVxfUqMeFP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}